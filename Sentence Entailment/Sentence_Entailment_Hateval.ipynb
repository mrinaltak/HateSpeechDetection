{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25QT54O2UjEh"
   },
   "source": [
    "Reference: https://github.com/dh1105/Sentence-Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YGhQEYnj7DD",
    "outputId": "819c882f-b0a1-4459-c424-d46e5be61b4f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uqcn3_lQ5Hhr",
    "outputId": "480b8d47-49e8-407d-cbb0-de91efbd2a15"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "## !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JQ4M8Znd4_ep"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkpfPDS1Z8a8",
    "outputId": "f7f15058-4a5a-44b3-a978-36a6f499f710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-eyFJM1SstoH"
   },
   "outputs": [],
   "source": [
    "task_a_hyp = \"This is hate speech.\"\n",
    "task_b_hyp = \"This is targeted hate.\"\n",
    "task_c_hyp = \"This is aggressive hate.\"\n",
    "# f = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/olid_train_v2.csv')\n",
    "# f_a = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/bert_nli.csv', 'w')\n",
    "f = open('Data/HatEval/hateval2019_en_train.csv')\n",
    "f_a = open('Data/HatEval/bert_nli_train.csv', 'w')\n",
    "f_a.write('tweet_id' + '\\t' + 'gold_label' + '\\t' + 'sentence1' + '\\t' + 'sentence2' + '\\n')\n",
    "lines = f.readlines()\n",
    "for line in lines[1:]:\n",
    "  row = line.split(',')\n",
    "  tweet_id = row[0].strip()\n",
    "  tweet_text = ','.join(row[1:-3]).strip()\n",
    "  is_hate = row[-3].strip()\n",
    "  is_targeted = row[-2].strip()\n",
    "  is_ag = row[-1].strip()\n",
    "  if is_hate == '0':\n",
    "    f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "  else:\n",
    "    f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "    if is_targeted == '0':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "    else:\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "    if is_ag == '0':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp + '\\n')\n",
    "    else:\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_c_hyp + '\\n')\n",
    "f.close()\n",
    "f_a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DCcdjRLDG3Ex"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/bert_nli.csv', delimiter='\\t')\n",
    "train_df = pd.read_csv('./Data/HatEval/bert_nli_train.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fbQWldbaIOGR"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>5859</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@SantinaDiMaggio happy birthday ya lil freak! ...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4332</th>\n",
       "      <td>2587</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>The biggest fear the Democrats have is Preside...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15330</th>\n",
       "      <td>8463</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>HAPPY BIRTHDAY YOU SKANK @spencerMR8 https://t...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15734</th>\n",
       "      <td>8750</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@kylegriffin1 Hard to figure which was the mor...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>1724</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Last week UNHCR released their annual Global T...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16351</th>\n",
       "      <td>9084</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@suma_surendran Wooow nace pic,, is bien you p...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14562</th>\n",
       "      <td>7695</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Sometimes I just wanna crash into psychic read...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13924</th>\n",
       "      <td>7057</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>RT @Hasto_McGasto: “@LACLlPPERS: #YesAllMen ht...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>6428</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@SapphireSux Sorry the old sapphire can’t come...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15565</th>\n",
       "      <td>8654</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@lloyd_frombriz @olgaNYC1211 @GOP @realDonaldT...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16566 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id     gold_label  \\\n",
       "11528      5859     entailment   \n",
       "4332       2587  contradiction   \n",
       "15330      8463  contradiction   \n",
       "15734      8750     entailment   \n",
       "2719       1724  contradiction   \n",
       "...         ...            ...   \n",
       "16351      9084     entailment   \n",
       "14562      7695  contradiction   \n",
       "13924      7057  contradiction   \n",
       "13231      6428     entailment   \n",
       "15565      8654  contradiction   \n",
       "\n",
       "                                               sentence1  \\\n",
       "11528  @SantinaDiMaggio happy birthday ya lil freak! ...   \n",
       "4332   The biggest fear the Democrats have is Preside...   \n",
       "15330  HAPPY BIRTHDAY YOU SKANK @spencerMR8 https://t...   \n",
       "15734  @kylegriffin1 Hard to figure which was the mor...   \n",
       "2719   Last week UNHCR released their annual Global T...   \n",
       "...                                                  ...   \n",
       "16351  @suma_surendran Wooow nace pic,, is bien you p...   \n",
       "14562  Sometimes I just wanna crash into psychic read...   \n",
       "13924  RT @Hasto_McGasto: “@LACLlPPERS: #YesAllMen ht...   \n",
       "13231  @SapphireSux Sorry the old sapphire can’t come...   \n",
       "15565  @lloyd_frombriz @olgaNYC1211 @GOP @realDonaldT...   \n",
       "\n",
       "                      sentence2  \n",
       "11528  This is aggressive hate.  \n",
       "4332       This is hate speech.  \n",
       "15330      This is hate speech.  \n",
       "15734    This is targeted hate.  \n",
       "2719       This is hate speech.  \n",
       "...                         ...  \n",
       "16351      This is hate speech.  \n",
       "14562      This is hate speech.  \n",
       "13924      This is hate speech.  \n",
       "13231      This is hate speech.  \n",
       "15565  This is aggressive hate.  \n",
       "\n",
       "[16566 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_a_hyp = \"This is hate speech.\"\n",
    "task_b_hyp = \"This is targeted hate.\"\n",
    "task_c_hyp = \"This is aggressive hate.\"\n",
    "# f = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/olid_train_v2.csv')\n",
    "# f_a = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/bert_nli.csv', 'w')\n",
    "f = open('Data/HatEval/hateval2019_en_dev.csv')\n",
    "f_a = open('Data/HatEval/bert_nli_val.csv', 'w')\n",
    "f_a.write('tweet_id' + '\\t' + 'gold_label' + '\\t' + 'sentence1' + '\\t' + 'sentence2' + '\\n')\n",
    "lines = f.readlines()\n",
    "for line in lines[1:]:\n",
    "  row = line.split(',')\n",
    "  tweet_id = row[0].strip()\n",
    "  tweet_text = ','.join(row[1:-3]).strip()\n",
    "  is_hate = row[-3].strip()\n",
    "  is_targeted = row[-2].strip()\n",
    "  is_ag = row[-1].strip()\n",
    "  if is_hate == '0':\n",
    "    f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "  else:\n",
    "    f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "    if is_targeted == '0':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "    else:\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "    if is_ag == '0':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp + '\\n')\n",
    "    else:\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_c_hyp + '\\n')\n",
    "f.close()\n",
    "f_a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('./Data/HatEval/bert_nli_val.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>18910</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@dumblefttweets @bryyelyy @tariqnasheed Stfu y...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>18385</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@Jim_Jordan Americans want @SpeakerRyan to fun...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>19107</td>\n",
       "      <td>entailment</td>\n",
       "      <td>\"Lol you're probably tweeting, \"\"it's so hard ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>18875</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@JKCorden Maybe James to You? All these Women ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>19103</td>\n",
       "      <td>entailment</td>\n",
       "      <td>shut the fuck up you know youre that bitch, whore</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>18519</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>We had plenty of diversity before the #Globali...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>18455</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Legal quirk often shortens prison time for cri...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>19168</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Ooh this is getting into Shakespearean levels ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>18771</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Bitch stfu</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>18465</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@TOAdamVaughan @JustinTrudeau @jyduclos Africa...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1854 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id     gold_label  \\\n",
       "1301     18910     entailment   \n",
       "327      18385  contradiction   \n",
       "1674     19107     entailment   \n",
       "1236     18875  contradiction   \n",
       "1668     19103     entailment   \n",
       "...        ...            ...   \n",
       "572      18519  contradiction   \n",
       "457      18455  contradiction   \n",
       "1787     19168  contradiction   \n",
       "1026     18771     entailment   \n",
       "477      18465  contradiction   \n",
       "\n",
       "                                              sentence1  \\\n",
       "1301  @dumblefttweets @bryyelyy @tariqnasheed Stfu y...   \n",
       "327   @Jim_Jordan Americans want @SpeakerRyan to fun...   \n",
       "1674  \"Lol you're probably tweeting, \"\"it's so hard ...   \n",
       "1236  @JKCorden Maybe James to You? All these Women ...   \n",
       "1668  shut the fuck up you know youre that bitch, whore   \n",
       "...                                                 ...   \n",
       "572   We had plenty of diversity before the #Globali...   \n",
       "457   Legal quirk often shortens prison time for cri...   \n",
       "1787  Ooh this is getting into Shakespearean levels ...   \n",
       "1026                                         Bitch stfu   \n",
       "477   @TOAdamVaughan @JustinTrudeau @jyduclos Africa...   \n",
       "\n",
       "                     sentence2  \n",
       "1301  This is aggressive hate.  \n",
       "327     This is targeted hate.  \n",
       "1674      This is hate speech.  \n",
       "1236      This is hate speech.  \n",
       "1668      This is hate speech.  \n",
       "...                        ...  \n",
       "572   This is aggressive hate.  \n",
       "457     This is targeted hate.  \n",
       "1787      This is hate speech.  \n",
       "1026      This is hate speech.  \n",
       "477     This is targeted hate.  \n",
       "\n",
       "[1854 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_a_hyp = \"This is hate speech.\"\n",
    "task_b_hyp = \"This is targeted hate.\"\n",
    "task_c_hyp = \"This is aggressive hate.\"\n",
    "# f = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/olid_train_v2.csv')\n",
    "# f_a = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/bert_nli.csv', 'w')\n",
    "f = open('Data/HatEval/hateval2019_en_test.csv')\n",
    "f_a = open('Data/HatEval/bert_nli_test.csv', 'w')\n",
    "f_a.write('tweet_id' + '\\t' + 'gold_label' + '\\t' + 'sentence1' + '\\t' + 'sentence2' + '\\n')\n",
    "lines = f.readlines()\n",
    "for line in lines[1:]:\n",
    "  row = line.split(',')\n",
    "  tweet_id = row[0].strip()\n",
    "  tweet_text = ','.join(row[1:-3]).strip()\n",
    "  is_hate = row[-3].strip()\n",
    "  is_targeted = row[-2].strip()\n",
    "  is_ag = row[-1].strip()\n",
    "  if is_hate == '0':\n",
    "    f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "  else:\n",
    "    f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "    if is_targeted == '0':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "    else:\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "    if is_ag == '0':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp + '\\n')\n",
    "    else:\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_c_hyp + '\\n')\n",
    "f.close()\n",
    "f_a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./Data/HatEval/bert_nli_test.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>30767</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Illegal Aliens Crossing Into America Isn't Imm...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>34261</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@triana922 The fact that they are here illegal...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>33011</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@realDonaldTrump Help #BuildThatWall Visit</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>30591</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Bitch idc when Ima see you hoe!!! https://t.co...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>31840</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>You gonna buy a whole ecosystem? is probably t...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>34358</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>W h o t h e H e l l are these People ? #Burnin...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>30484</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@JessicaV_CIS THIS IS FABULOUS! I HOPE THE ACL...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>30019</td>\n",
       "      <td>entailment</td>\n",
       "      <td>When one of your old hoes dm you saying '' oh ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>31935</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@RealJamesWoods WE CAN NOT forget to vote.Plen...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>33726</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Bitch, fall in love with edm doesn't mean you ...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5520 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id     gold_label  \\\n",
       "1906     30767     entailment   \n",
       "927      34261  contradiction   \n",
       "2101     33011  contradiction   \n",
       "4410     30591     entailment   \n",
       "3397     31840  contradiction   \n",
       "...        ...            ...   \n",
       "284      34358  contradiction   \n",
       "2147     30484  contradiction   \n",
       "4737     30019     entailment   \n",
       "2105     31935  contradiction   \n",
       "3841     33726     entailment   \n",
       "\n",
       "                                              sentence1  \\\n",
       "1906  Illegal Aliens Crossing Into America Isn't Imm...   \n",
       "927   @triana922 The fact that they are here illegal...   \n",
       "2101         @realDonaldTrump Help #BuildThatWall Visit   \n",
       "4410  Bitch idc when Ima see you hoe!!! https://t.co...   \n",
       "3397  You gonna buy a whole ecosystem? is probably t...   \n",
       "...                                                 ...   \n",
       "284   W h o t h e H e l l are these People ? #Burnin...   \n",
       "2147  @JessicaV_CIS THIS IS FABULOUS! I HOPE THE ACL...   \n",
       "4737  When one of your old hoes dm you saying '' oh ...   \n",
       "2105  @RealJamesWoods WE CAN NOT forget to vote.Plen...   \n",
       "3841  Bitch, fall in love with edm doesn't mean you ...   \n",
       "\n",
       "                     sentence2  \n",
       "1906  This is aggressive hate.  \n",
       "927   This is aggressive hate.  \n",
       "2101      This is hate speech.  \n",
       "4410      This is hate speech.  \n",
       "3397      This is hate speech.  \n",
       "...                        ...  \n",
       "284   This is aggressive hate.  \n",
       "2147      This is hate speech.  \n",
       "4737      This is hate speech.  \n",
       "2105      This is hate speech.  \n",
       "3841    This is targeted hate.  \n",
       "\n",
       "[5520 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3NgPYXBg4_e7"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UMTQb1iMF54W"
   },
   "outputs": [],
   "source": [
    "train_df['sentence1'] = train_df['sentence1'].astype(str)\n",
    "train_df['sentence2'] = train_df['sentence2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GMzPNmtpF8yg"
   },
   "outputs": [],
   "source": [
    "val_df['sentence1'] = val_df['sentence1'].astype(str)\n",
    "val_df['sentence2'] = val_df['sentence2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6UBcmYsPzWA7"
   },
   "outputs": [],
   "source": [
    "test_df['sentence1'] = test_df['sentence1'].astype(str)\n",
    "test_df['sentence2'] = test_df['sentence2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "diS3wCm14_e9"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]\n",
    "val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]\n",
    "test_df = test_df[(test_df['sentence1'].str.split().str.len() > 0) & (test_df['sentence2'].str.split().str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WinUrXE04_e9",
    "outputId": "6e4a47fe-cad1-4b40-c83f-acef182072c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>5859</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@SantinaDiMaggio happy birthday ya lil freak! ...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4332</th>\n",
       "      <td>2587</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>The biggest fear the Democrats have is Preside...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15330</th>\n",
       "      <td>8463</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>HAPPY BIRTHDAY YOU SKANK @spencerMR8 https://t...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15734</th>\n",
       "      <td>8750</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@kylegriffin1 Hard to figure which was the mor...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>1724</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Last week UNHCR released their annual Global T...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16351</th>\n",
       "      <td>9084</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@suma_surendran Wooow nace pic,, is bien you p...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14562</th>\n",
       "      <td>7695</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Sometimes I just wanna crash into psychic read...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13924</th>\n",
       "      <td>7057</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>RT @Hasto_McGasto: “@LACLlPPERS: #YesAllMen ht...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>6428</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@SapphireSux Sorry the old sapphire can’t come...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15565</th>\n",
       "      <td>8654</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@lloyd_frombriz @olgaNYC1211 @GOP @realDonaldT...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16566 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id     gold_label  \\\n",
       "11528      5859     entailment   \n",
       "4332       2587  contradiction   \n",
       "15330      8463  contradiction   \n",
       "15734      8750     entailment   \n",
       "2719       1724  contradiction   \n",
       "...         ...            ...   \n",
       "16351      9084     entailment   \n",
       "14562      7695  contradiction   \n",
       "13924      7057  contradiction   \n",
       "13231      6428     entailment   \n",
       "15565      8654  contradiction   \n",
       "\n",
       "                                               sentence1  \\\n",
       "11528  @SantinaDiMaggio happy birthday ya lil freak! ...   \n",
       "4332   The biggest fear the Democrats have is Preside...   \n",
       "15330  HAPPY BIRTHDAY YOU SKANK @spencerMR8 https://t...   \n",
       "15734  @kylegriffin1 Hard to figure which was the mor...   \n",
       "2719   Last week UNHCR released their annual Global T...   \n",
       "...                                                  ...   \n",
       "16351  @suma_surendran Wooow nace pic,, is bien you p...   \n",
       "14562  Sometimes I just wanna crash into psychic read...   \n",
       "13924  RT @Hasto_McGasto: “@LACLlPPERS: #YesAllMen ht...   \n",
       "13231  @SapphireSux Sorry the old sapphire can’t come...   \n",
       "15565  @lloyd_frombriz @olgaNYC1211 @GOP @realDonaldT...   \n",
       "\n",
       "                      sentence2  \n",
       "11528  This is aggressive hate.  \n",
       "4332       This is hate speech.  \n",
       "15330      This is hate speech.  \n",
       "15734    This is targeted hate.  \n",
       "2719       This is hate speech.  \n",
       "...                         ...  \n",
       "16351      This is hate speech.  \n",
       "14562      This is hate speech.  \n",
       "13924      This is hate speech.  \n",
       "13231      This is hate speech.  \n",
       "15565  This is aggressive hate.  \n",
       "\n",
       "[16566 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6e4dxTks4_e-",
    "outputId": "94ec80ad-9e97-49bf-8b63-4fef22a25fa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>18910</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@dumblefttweets @bryyelyy @tariqnasheed Stfu y...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>18385</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@Jim_Jordan Americans want @SpeakerRyan to fun...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>19107</td>\n",
       "      <td>entailment</td>\n",
       "      <td>\"Lol you're probably tweeting, \"\"it's so hard ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>18875</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@JKCorden Maybe James to You? All these Women ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>19103</td>\n",
       "      <td>entailment</td>\n",
       "      <td>shut the fuck up you know youre that bitch, whore</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>18519</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>We had plenty of diversity before the #Globali...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>18455</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Legal quirk often shortens prison time for cri...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>19168</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Ooh this is getting into Shakespearean levels ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>18771</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Bitch stfu</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>18465</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@TOAdamVaughan @JustinTrudeau @jyduclos Africa...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1854 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id     gold_label  \\\n",
       "1301     18910     entailment   \n",
       "327      18385  contradiction   \n",
       "1674     19107     entailment   \n",
       "1236     18875  contradiction   \n",
       "1668     19103     entailment   \n",
       "...        ...            ...   \n",
       "572      18519  contradiction   \n",
       "457      18455  contradiction   \n",
       "1787     19168  contradiction   \n",
       "1026     18771     entailment   \n",
       "477      18465  contradiction   \n",
       "\n",
       "                                              sentence1  \\\n",
       "1301  @dumblefttweets @bryyelyy @tariqnasheed Stfu y...   \n",
       "327   @Jim_Jordan Americans want @SpeakerRyan to fun...   \n",
       "1674  \"Lol you're probably tweeting, \"\"it's so hard ...   \n",
       "1236  @JKCorden Maybe James to You? All these Women ...   \n",
       "1668  shut the fuck up you know youre that bitch, whore   \n",
       "...                                                 ...   \n",
       "572   We had plenty of diversity before the #Globali...   \n",
       "457   Legal quirk often shortens prison time for cri...   \n",
       "1787  Ooh this is getting into Shakespearean levels ...   \n",
       "1026                                         Bitch stfu   \n",
       "477   @TOAdamVaughan @JustinTrudeau @jyduclos Africa...   \n",
       "\n",
       "                     sentence2  \n",
       "1301  This is aggressive hate.  \n",
       "327     This is targeted hate.  \n",
       "1674      This is hate speech.  \n",
       "1236      This is hate speech.  \n",
       "1668      This is hate speech.  \n",
       "...                        ...  \n",
       "572   This is aggressive hate.  \n",
       "457     This is targeted hate.  \n",
       "1787      This is hate speech.  \n",
       "1026      This is hate speech.  \n",
       "477     This is targeted hate.  \n",
       "\n",
       "[1854 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-nQRV3wZzlMe",
    "outputId": "601de039-4efc-4156-b7c6-364876406bba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>30767</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Illegal Aliens Crossing Into America Isn't Imm...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>34261</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@triana922 The fact that they are here illegal...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>33011</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@realDonaldTrump Help #BuildThatWall Visit</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>30591</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Bitch idc when Ima see you hoe!!! https://t.co...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>31840</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>You gonna buy a whole ecosystem? is probably t...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>34358</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>W h o t h e H e l l are these People ? #Burnin...</td>\n",
       "      <td>This is aggressive hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>30484</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@JessicaV_CIS THIS IS FABULOUS! I HOPE THE ACL...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>30019</td>\n",
       "      <td>entailment</td>\n",
       "      <td>When one of your old hoes dm you saying '' oh ...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>31935</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@RealJamesWoods WE CAN NOT forget to vote.Plen...</td>\n",
       "      <td>This is hate speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>33726</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Bitch, fall in love with edm doesn't mean you ...</td>\n",
       "      <td>This is targeted hate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5520 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id     gold_label  \\\n",
       "1906     30767     entailment   \n",
       "927      34261  contradiction   \n",
       "2101     33011  contradiction   \n",
       "4410     30591     entailment   \n",
       "3397     31840  contradiction   \n",
       "...        ...            ...   \n",
       "284      34358  contradiction   \n",
       "2147     30484  contradiction   \n",
       "4737     30019     entailment   \n",
       "2105     31935  contradiction   \n",
       "3841     33726     entailment   \n",
       "\n",
       "                                              sentence1  \\\n",
       "1906  Illegal Aliens Crossing Into America Isn't Imm...   \n",
       "927   @triana922 The fact that they are here illegal...   \n",
       "2101         @realDonaldTrump Help #BuildThatWall Visit   \n",
       "4410  Bitch idc when Ima see you hoe!!! https://t.co...   \n",
       "3397  You gonna buy a whole ecosystem? is probably t...   \n",
       "...                                                 ...   \n",
       "284   W h o t h e H e l l are these People ? #Burnin...   \n",
       "2147  @JessicaV_CIS THIS IS FABULOUS! I HOPE THE ACL...   \n",
       "4737  When one of your old hoes dm you saying '' oh ...   \n",
       "2105  @RealJamesWoods WE CAN NOT forget to vote.Plen...   \n",
       "3841  Bitch, fall in love with edm doesn't mean you ...   \n",
       "\n",
       "                     sentence2  \n",
       "1906  This is aggressive hate.  \n",
       "927   This is aggressive hate.  \n",
       "2101      This is hate speech.  \n",
       "4410      This is hate speech.  \n",
       "3397      This is hate speech.  \n",
       "...                        ...  \n",
       "284   This is aggressive hate.  \n",
       "2147      This is hate speech.  \n",
       "4737      This is hate speech.  \n",
       "2105      This is hate speech.  \n",
       "3841    This is targeted hate.  \n",
       "\n",
       "[5520 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "P11Q5BRkX70D"
   },
   "outputs": [],
   "source": [
    "test_df_a = test_df[test_df.sentence2 == \"This is hate speech.\"]\n",
    "test_df_b = test_df[test_df.sentence2 == \"This is targeted hate.\"]\n",
    "test_df_c = test_df[test_df.sentence2 == \"This is aggressive hate.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSjkMi9VZ4N2",
    "outputId": "5f824c54-3425-48d2-faa7-a7f85693565c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "1260\n",
      "1260\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df_a))\n",
    "print(len(test_df_b))\n",
    "print(len(test_df_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Gk96lNh94_e_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df, test_df_a, test_df_b, test_df_c):\n",
    "    self.label_dict = {'entailment': 0, 'contradiction': 1}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "    self.test_df_a = test_df_a\n",
    "    self.test_df_b = test_df_b\n",
    "    self.test_df_c = test_df_c\n",
    "    self.base_path = 'mnli-data-hateval'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.test_data_a = None\n",
    "    self.test_data_b = None\n",
    "    self.test_data_c = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    #Saving takes too much RAM\n",
    "    \n",
    "    if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\n",
    "      print(\"Found training data\")\n",
    "      with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\n",
    "        self.train_data = pickle.load(f)\n",
    "    else:\n",
    "      self.train_data = self.load_data(self.train_df)\n",
    "      with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.train_data, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\n",
    "      print(\"Found val data\")\n",
    "      with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\n",
    "        self.val_data = pickle.load(f)\n",
    "    else:\n",
    "      self.val_data = self.load_data(self.val_df)\n",
    "      with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.val_data, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data.pkl')):\n",
    "      print(\"Found test data\")\n",
    "      with open(os.path.join(self.base_path, 'test_data.pkl'), 'rb') as f:\n",
    "        self.test_data = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data = self.load_data(self.test_df)\n",
    "      with open(os.path.join(self.base_path, 'test_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_a.pkl')):\n",
    "      print(\"Found test data a\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_a.pkl'), 'rb') as f:\n",
    "        self.test_data_a = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_a = self.load_data(self.test_df_a)\n",
    "      with open(os.path.join(self.base_path, 'test_data_a.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_a, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_b.pkl')):\n",
    "      print(\"Found test data b\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_b.pkl'), 'rb') as f:\n",
    "        self.test_data_b = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_b = self.load_data(self.test_df_b)\n",
    "      with open(os.path.join(self.base_path, 'test_data_b.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_b, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_c.pkl')):\n",
    "      print(\"Found test data c\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_c.pkl'), 'rb') as f:\n",
    "        self.test_data_c = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_c = self.load_data(self.test_df_c)\n",
    "      with open(os.path.join(self.base_path, 'test_data_c.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_c, f)\n",
    "    # self.train_data = self.load_data(self.train_df)\n",
    "    # self.val_data = self.load_data(self.val_df)\n",
    "    # self.test_data = self.load_data(self.test_df)\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['sentence1'].to_list()\n",
    "    hypothesis_list = df['sentence2'].to_list()\n",
    "    label_list = df['gold_label'].to_list()\n",
    "\n",
    "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_a = DataLoader(\n",
    "      self.test_data_a,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_b = DataLoader(\n",
    "      self.test_data_b,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_c = DataLoader(\n",
    "      self.test_data_c,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, test_loader_a, test_loader_b, test_loader_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "6c0f5454779b4baeafa828e961459116",
      "f17b3761a665477e8fdecfd23d267f87",
      "a39de3d6df814e98be1a65e049bff39e",
      "ef706620a1e34c3a84de18a03a85f8cd",
      "bfee36d5b72d469cb308f4007e1474a0",
      "f3dea62699df437c945160dfc42afbd5",
      "24f1399337dc4cf998e189d697b8e282",
      "ff8facd357854c18976dea8f1621cae6",
      "e0c07e3c6bd042c2a3dd2aed325f6759",
      "4720c540eecc4dbe8fee8c9596f56933",
      "dcde074381814577a68122dd6e5adf30",
      "95d0e5c35a01408fa9bde956d0785d21",
      "93ded7574b56441eaa9b6aaaa30840f3",
      "405ea21b2692487298e89e719c6573b0",
      "0183e10141f049a1820827c9dfbd6c5a",
      "8665ae01129141dfb7fdf5037a994c86",
      "16b6fa8b34e24eedb7c6c80e9e57b5d2",
      "dd90fde29e26447c9eaeef9aaf15d01f",
      "8a4784a513b749d68959b4250a8a1593",
      "d01f56954ef74452b649def7b9adc937",
      "a61965e1ea50438c868778deb83db312",
      "93f0f8c7257944e6ab97295f09821f1d",
      "c38a9f17f85544b68ffdc8e501a95f20",
      "41fe1073e1ea44669cdbef98949f9528",
      "45dcdc23c9b4481a9e40878e4dbfd585",
      "322cc0d27aea4458a9efe1c099943482",
      "e4669478d1474c4485c45907447d75a5",
      "d09c0639c5ef45a7823074eb648de6d7",
      "f2a5689111c3420c9ed4114bb55cf0fc",
      "677cf9fc8f3d47c68e73bd4a3622132b",
      "e0d781c1aae342ae949264fb561b7f5f",
      "602d9b5979084873996eab9784b98edc",
      "546361d1061243f5b7fc5692b1c9d4c5",
      "30c1af9f395f4f148a4038c1f13b2088",
      "1543919f31dd46c6a967a412af4d21bd",
      "7e7c6e812fdc4b36a25096e9f176f75a",
      "bf7f2daca2264bd289670898d7ab10e4",
      "3af999385a094ccc85eadf702ab69723",
      "d84069b7b1384beaa5100505f9233b2b",
      "2e8aaf8e2a644b8384d07e86578e7624",
      "89a7fc4f932041a7b76b2502096ede49",
      "5d80c6db4dfc4bf0aaea82350dffdc21",
      "c2496080cc384158856247dd5581e751",
      "3e50074e3a054225842a6fe8ef936183"
     ]
    },
    "id": "md52P1z14_e_",
    "outputId": "e875db6f-6bb9-44a1-f2de-0e8cbea3e883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found training data\n",
      "Found val data\n",
      "Found test data\n",
      "Found test data a\n",
      "Found test data b\n",
      "Found test data c\n"
     ]
    }
   ],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df, test_df_a, test_df_b, test_df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-LSyABLG4_fA"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, test_loader_a, test_loader_b, test_loader_c = mnli_dataset.get_data_loaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e8a66bb5d7614602ac93db9cac811135",
      "0504a8ce7be543a49611a424fcfc9ca7",
      "bf065a9136dc4262a74c04f746b7c143",
      "6f61782328964e4dbdea05bee3797ee9",
      "814534237a9144deb7d52e4ceffabc43",
      "43f942f812884ace90aa9b1dd41a3d9a",
      "9d383d03f9304bce914326ea0e5d472b",
      "6b127ca5fb5a4d23b6c3ffe654cdfcaf",
      "fb792d8fbadc4bb6977a1986a9783e15",
      "373af73466be45f1bb216e6abbdb15be",
      "6497737147ce458b881bbb1b17492364"
     ]
    },
    "id": "qOdc4Cs2DEjt",
    "outputId": "7cdbd55e-a6e6-4cad-e765-de1a18e3f0c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jxzpENXlEh-u"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "is1TqwTREid9"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79aI1dun4_fB",
    "outputId": "eb5cbe4e-f9fa-4618-d840-611d259037f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,483,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qhU855Cw4_fB"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OfhYO7Db4_fB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):  \n",
    "  total_step = len(train_loader)\n",
    "  #model.load_state_dict(torch.load('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Weights/nli_0.pt'))\n",
    "  #model.load_state_dict(torch.load('./WeightsHateval/nli_4.pt'))\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      print(batch_idx, len(train_loader))\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        # loss = criterion(prediction, labels)\n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    #torch.save(model.state_dict(),os.path.join('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Weights', 'nli_{}.pt'.format(epoch)))\n",
    "    torch.save(model.state_dict(),os.path.join('./WeightsHateval', 'nli_{}.pt'.format(epoch)))\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "TisZ5o86nkMW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def test(model, test_loader):\n",
    "  model.load_state_dict(torch.load('WeightsHateval/nli_1.pt', map_location=torch.device('cpu')))\n",
    "  model.eval()\n",
    "  total_test_acc  = 0\n",
    "  total_test_loss = 0\n",
    "  start = time.time()\n",
    "  preds = []\n",
    "  acts = []\n",
    "  with torch.no_grad():\n",
    "    #print(\"Prediction\", \"Label\")\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(test_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                           token_type_ids=seg_ids, \n",
    "                           attention_mask=mask_ids, \n",
    "                           labels=labels).values()\n",
    "        \n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "      preds = preds + prediction.tolist()\n",
    "      acts = acts + labels.tolist()\n",
    "      #print(prediction, labels)\n",
    "      total_test_loss += loss.item()\n",
    "      total_test_acc  += acc.item() \n",
    "\n",
    "  test_acc  = total_test_acc/len(test_loader)\n",
    "  test_loss = total_test_loss/len(test_loader)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "  print(f'test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  return [0 if preds[i][0] > preds[i][1] else 1 for i in range(len(preds))], acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPIxT4RB4_fC",
    "outputId": "9af2f2cb-4099-4673-f834-18bcc9bfcac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 518\n",
      "1 518\n",
      "2 518\n",
      "3 518\n",
      "4 518\n",
      "5 518\n",
      "6 518\n",
      "7 518\n",
      "8 518\n",
      "9 518\n",
      "10 518\n",
      "11 518\n",
      "12 518\n",
      "13 518\n",
      "14 518\n",
      "15 518\n",
      "16 518\n",
      "17 518\n",
      "18 518\n",
      "19 518\n",
      "20 518\n",
      "21 518\n",
      "22 518\n",
      "23 518\n",
      "24 518\n",
      "25 518\n",
      "26 518\n",
      "27 518\n",
      "28 518\n",
      "29 518\n",
      "30 518\n",
      "31 518\n",
      "32 518\n",
      "33 518\n",
      "34 518\n",
      "35 518\n",
      "36 518\n",
      "37 518\n",
      "38 518\n",
      "39 518\n",
      "40 518\n",
      "41 518\n",
      "42 518\n",
      "43 518\n",
      "44 518\n",
      "45 518\n",
      "46 518\n",
      "47 518\n",
      "48 518\n",
      "49 518\n",
      "50 518\n",
      "51 518\n",
      "52 518\n",
      "53 518\n",
      "54 518\n",
      "55 518\n",
      "56 518\n",
      "57 518\n",
      "58 518\n",
      "59 518\n",
      "60 518\n",
      "61 518\n",
      "62 518\n",
      "63 518\n",
      "64 518\n",
      "65 518\n",
      "66 518\n",
      "67 518\n",
      "68 518\n",
      "69 518\n",
      "70 518\n",
      "71 518\n",
      "72 518\n",
      "73 518\n",
      "74 518\n",
      "75 518\n",
      "76 518\n",
      "77 518\n",
      "78 518\n",
      "79 518\n",
      "80 518\n",
      "81 518\n",
      "82 518\n",
      "83 518\n",
      "84 518\n",
      "85 518\n",
      "86 518\n",
      "87 518\n",
      "88 518\n",
      "89 518\n",
      "90 518\n",
      "91 518\n",
      "92 518\n",
      "93 518\n",
      "94 518\n",
      "95 518\n",
      "96 518\n",
      "97 518\n",
      "98 518\n",
      "99 518\n",
      "100 518\n",
      "101 518\n",
      "102 518\n",
      "103 518\n",
      "104 518\n",
      "105 518\n",
      "106 518\n",
      "107 518\n",
      "108 518\n",
      "109 518\n",
      "110 518\n",
      "111 518\n",
      "112 518\n",
      "113 518\n",
      "114 518\n",
      "115 518\n",
      "116 518\n",
      "117 518\n",
      "118 518\n",
      "119 518\n",
      "120 518\n",
      "121 518\n",
      "122 518\n",
      "123 518\n",
      "124 518\n",
      "125 518\n",
      "126 518\n",
      "127 518\n",
      "128 518\n",
      "129 518\n",
      "130 518\n",
      "131 518\n",
      "132 518\n",
      "133 518\n",
      "134 518\n",
      "135 518\n",
      "136 518\n",
      "137 518\n",
      "138 518\n",
      "139 518\n",
      "140 518\n",
      "141 518\n",
      "142 518\n",
      "143 518\n",
      "144 518\n",
      "145 518\n",
      "146 518\n",
      "147 518\n",
      "148 518\n",
      "149 518\n",
      "150 518\n",
      "151 518\n",
      "152 518\n",
      "153 518\n",
      "154 518\n",
      "155 518\n",
      "156 518\n",
      "157 518\n",
      "158 518\n",
      "159 518\n",
      "160 518\n",
      "161 518\n",
      "162 518\n",
      "163 518\n",
      "164 518\n",
      "165 518\n",
      "166 518\n",
      "167 518\n",
      "168 518\n",
      "169 518\n",
      "170 518\n",
      "171 518\n",
      "172 518\n",
      "173 518\n",
      "174 518\n",
      "175 518\n",
      "176 518\n",
      "177 518\n",
      "178 518\n",
      "179 518\n",
      "180 518\n",
      "181 518\n",
      "182 518\n",
      "183 518\n",
      "184 518\n",
      "185 518\n",
      "186 518\n",
      "187 518\n",
      "188 518\n",
      "189 518\n",
      "190 518\n",
      "191 518\n",
      "192 518\n",
      "193 518\n",
      "194 518\n",
      "195 518\n",
      "196 518\n",
      "197 518\n",
      "198 518\n",
      "199 518\n",
      "200 518\n",
      "201 518\n",
      "202 518\n",
      "203 518\n",
      "204 518\n",
      "205 518\n",
      "206 518\n",
      "207 518\n",
      "208 518\n",
      "209 518\n",
      "210 518\n",
      "211 518\n",
      "212 518\n",
      "213 518\n",
      "214 518\n",
      "215 518\n",
      "216 518\n",
      "217 518\n",
      "218 518\n",
      "219 518\n",
      "220 518\n",
      "221 518\n",
      "222 518\n",
      "223 518\n",
      "224 518\n",
      "225 518\n",
      "226 518\n",
      "227 518\n",
      "228 518\n",
      "229 518\n",
      "230 518\n",
      "231 518\n",
      "232 518\n",
      "233 518\n",
      "234 518\n",
      "235 518\n",
      "236 518\n",
      "237 518\n",
      "238 518\n",
      "239 518\n",
      "240 518\n",
      "241 518\n",
      "242 518\n",
      "243 518\n",
      "244 518\n",
      "245 518\n",
      "246 518\n",
      "247 518\n",
      "248 518\n",
      "249 518\n",
      "250 518\n",
      "251 518\n",
      "252 518\n",
      "253 518\n",
      "254 518\n",
      "255 518\n",
      "256 518\n",
      "257 518\n",
      "258 518\n",
      "259 518\n",
      "260 518\n",
      "261 518\n",
      "262 518\n",
      "263 518\n",
      "264 518\n",
      "265 518\n",
      "266 518\n",
      "267 518\n",
      "268 518\n",
      "269 518\n",
      "270 518\n",
      "271 518\n",
      "272 518\n",
      "273 518\n",
      "274 518\n",
      "275 518\n",
      "276 518\n",
      "277 518\n",
      "278 518\n",
      "279 518\n",
      "280 518\n",
      "281 518\n",
      "282 518\n",
      "283 518\n",
      "284 518\n",
      "285 518\n",
      "286 518\n",
      "287 518\n",
      "288 518\n",
      "289 518\n",
      "290 518\n",
      "291 518\n",
      "292 518\n",
      "293 518\n",
      "294 518\n",
      "295 518\n",
      "296 518\n",
      "297 518\n",
      "298 518\n",
      "299 518\n",
      "300 518\n",
      "301 518\n",
      "302 518\n",
      "303 518\n",
      "304 518\n",
      "305 518\n",
      "306 518\n",
      "307 518\n",
      "308 518\n",
      "309 518\n",
      "310 518\n",
      "311 518\n",
      "312 518\n",
      "313 518\n",
      "314 518\n",
      "315 518\n",
      "316 518\n",
      "317 518\n",
      "318 518\n",
      "319 518\n",
      "320 518\n",
      "321 518\n",
      "322 518\n",
      "323 518\n",
      "324 518\n",
      "325 518\n",
      "326 518\n",
      "327 518\n",
      "328 518\n",
      "329 518\n",
      "330 518\n",
      "331 518\n",
      "332 518\n",
      "333 518\n",
      "334 518\n",
      "335 518\n",
      "336 518\n",
      "337 518\n",
      "338 518\n",
      "339 518\n",
      "340 518\n",
      "341 518\n",
      "342 518\n",
      "343 518\n",
      "344 518\n",
      "345 518\n",
      "346 518\n",
      "347 518\n",
      "348 518\n",
      "349 518\n",
      "350 518\n",
      "351 518\n",
      "352 518\n",
      "353 518\n",
      "354 518\n",
      "355 518\n",
      "356 518\n",
      "357 518\n",
      "358 518\n",
      "359 518\n",
      "360 518\n",
      "361 518\n",
      "362 518\n",
      "363 518\n",
      "364 518\n",
      "365 518\n",
      "366 518\n",
      "367 518\n",
      "368 518\n",
      "369 518\n",
      "370 518\n",
      "371 518\n",
      "372 518\n",
      "373 518\n",
      "374 518\n",
      "375 518\n",
      "376 518\n",
      "377 518\n",
      "378 518\n",
      "379 518\n",
      "380 518\n",
      "381 518\n",
      "382 518\n",
      "383 518\n",
      "384 518\n",
      "385 518\n",
      "386 518\n",
      "387 518\n",
      "388 518\n",
      "389 518\n",
      "390 518\n",
      "391 518\n",
      "392 518\n",
      "393 518\n",
      "394 518\n",
      "395 518\n",
      "396 518\n",
      "397 518\n",
      "398 518\n",
      "399 518\n",
      "400 518\n",
      "401 518\n",
      "402 518\n",
      "403 518\n",
      "404 518\n",
      "405 518\n",
      "406 518\n",
      "407 518\n",
      "408 518\n",
      "409 518\n",
      "410 518\n",
      "411 518\n",
      "412 518\n",
      "413 518\n",
      "414 518\n",
      "415 518\n",
      "416 518\n",
      "417 518\n",
      "418 518\n",
      "419 518\n",
      "420 518\n",
      "421 518\n",
      "422 518\n",
      "423 518\n",
      "424 518\n",
      "425 518\n",
      "426 518\n",
      "427 518\n",
      "428 518\n",
      "429 518\n",
      "430 518\n",
      "431 518\n",
      "432 518\n",
      "433 518\n",
      "434 518\n",
      "435 518\n",
      "436 518\n",
      "437 518\n",
      "438 518\n",
      "439 518\n",
      "440 518\n",
      "441 518\n",
      "442 518\n",
      "443 518\n",
      "444 518\n",
      "445 518\n",
      "446 518\n",
      "447 518\n",
      "448 518\n",
      "449 518\n",
      "450 518\n",
      "451 518\n",
      "452 518\n",
      "453 518\n",
      "454 518\n",
      "455 518\n",
      "456 518\n",
      "457 518\n",
      "458 518\n",
      "459 518\n",
      "460 518\n",
      "461 518\n",
      "462 518\n",
      "463 518\n",
      "464 518\n",
      "465 518\n",
      "466 518\n",
      "467 518\n",
      "468 518\n",
      "469 518\n",
      "470 518\n",
      "471 518\n",
      "472 518\n",
      "473 518\n",
      "474 518\n",
      "475 518\n",
      "476 518\n",
      "477 518\n",
      "478 518\n",
      "479 518\n",
      "480 518\n",
      "481 518\n",
      "482 518\n",
      "483 518\n",
      "484 518\n",
      "485 518\n",
      "486 518\n",
      "487 518\n",
      "488 518\n",
      "489 518\n",
      "490 518\n",
      "491 518\n",
      "492 518\n",
      "493 518\n",
      "494 518\n",
      "495 518\n",
      "496 518\n",
      "497 518\n",
      "498 518\n",
      "499 518\n",
      "500 518\n",
      "501 518\n",
      "502 518\n",
      "503 518\n",
      "504 518\n",
      "505 518\n",
      "506 518\n",
      "507 518\n",
      "508 518\n",
      "509 518\n",
      "510 518\n",
      "511 518\n",
      "512 518\n",
      "513 518\n",
      "514 518\n",
      "515 518\n",
      "516 518\n",
      "517 518\n",
      "Epoch 1: train_loss: 0.5150 train_acc: 0.7320 | val_loss: 0.4720 val_acc: 0.7654\n",
      "00:06:32.95\n",
      "0 518\n",
      "1 518\n",
      "2 518\n",
      "3 518\n",
      "4 518\n",
      "5 518\n",
      "6 518\n",
      "7 518\n",
      "8 518\n",
      "9 518\n",
      "10 518\n",
      "11 518\n",
      "12 518\n",
      "13 518\n",
      "14 518\n",
      "15 518\n",
      "16 518\n",
      "17 518\n",
      "18 518\n",
      "19 518\n",
      "20 518\n",
      "21 518\n",
      "22 518\n",
      "23 518\n",
      "24 518\n",
      "25 518\n",
      "26 518\n",
      "27 518\n",
      "28 518\n",
      "29 518\n",
      "30 518\n",
      "31 518\n",
      "32 518\n",
      "33 518\n",
      "34 518\n",
      "35 518\n",
      "36 518\n",
      "37 518\n",
      "38 518\n",
      "39 518\n",
      "40 518\n",
      "41 518\n",
      "42 518\n",
      "43 518\n",
      "44 518\n",
      "45 518\n",
      "46 518\n",
      "47 518\n",
      "48 518\n",
      "49 518\n",
      "50 518\n",
      "51 518\n",
      "52 518\n",
      "53 518\n",
      "54 518\n",
      "55 518\n",
      "56 518\n",
      "57 518\n",
      "58 518\n",
      "59 518\n",
      "60 518\n",
      "61 518\n",
      "62 518\n",
      "63 518\n",
      "64 518\n",
      "65 518\n",
      "66 518\n",
      "67 518\n",
      "68 518\n",
      "69 518\n",
      "70 518\n",
      "71 518\n",
      "72 518\n",
      "73 518\n",
      "74 518\n",
      "75 518\n",
      "76 518\n",
      "77 518\n",
      "78 518\n",
      "79 518\n",
      "80 518\n",
      "81 518\n",
      "82 518\n",
      "83 518\n",
      "84 518\n",
      "85 518\n",
      "86 518\n",
      "87 518\n",
      "88 518\n",
      "89 518\n",
      "90 518\n",
      "91 518\n",
      "92 518\n",
      "93 518\n",
      "94 518\n",
      "95 518\n",
      "96 518\n",
      "97 518\n",
      "98 518\n",
      "99 518\n",
      "100 518\n",
      "101 518\n",
      "102 518\n",
      "103 518\n",
      "104 518\n",
      "105 518\n",
      "106 518\n",
      "107 518\n",
      "108 518\n",
      "109 518\n",
      "110 518\n",
      "111 518\n",
      "112 518\n",
      "113 518\n",
      "114 518\n",
      "115 518\n",
      "116 518\n",
      "117 518\n",
      "118 518\n",
      "119 518\n",
      "120 518\n",
      "121 518\n",
      "122 518\n",
      "123 518\n",
      "124 518\n",
      "125 518\n",
      "126 518\n",
      "127 518\n",
      "128 518\n",
      "129 518\n",
      "130 518\n",
      "131 518\n",
      "132 518\n",
      "133 518\n",
      "134 518\n",
      "135 518\n",
      "136 518\n",
      "137 518\n",
      "138 518\n",
      "139 518\n",
      "140 518\n",
      "141 518\n",
      "142 518\n",
      "143 518\n",
      "144 518\n",
      "145 518\n",
      "146 518\n",
      "147 518\n",
      "148 518\n",
      "149 518\n",
      "150 518\n",
      "151 518\n",
      "152 518\n",
      "153 518\n",
      "154 518\n",
      "155 518\n",
      "156 518\n",
      "157 518\n",
      "158 518\n",
      "159 518\n",
      "160 518\n",
      "161 518\n",
      "162 518\n",
      "163 518\n",
      "164 518\n",
      "165 518\n",
      "166 518\n",
      "167 518\n",
      "168 518\n",
      "169 518\n",
      "170 518\n",
      "171 518\n",
      "172 518\n",
      "173 518\n",
      "174 518\n",
      "175 518\n",
      "176 518\n",
      "177 518\n",
      "178 518\n",
      "179 518\n",
      "180 518\n",
      "181 518\n",
      "182 518\n",
      "183 518\n",
      "184 518\n",
      "185 518\n",
      "186 518\n",
      "187 518\n",
      "188 518\n",
      "189 518\n",
      "190 518\n",
      "191 518\n",
      "192 518\n",
      "193 518\n",
      "194 518\n",
      "195 518\n",
      "196 518\n",
      "197 518\n",
      "198 518\n",
      "199 518\n",
      "200 518\n",
      "201 518\n",
      "202 518\n",
      "203 518\n",
      "204 518\n",
      "205 518\n",
      "206 518\n",
      "207 518\n",
      "208 518\n",
      "209 518\n",
      "210 518\n",
      "211 518\n",
      "212 518\n",
      "213 518\n",
      "214 518\n",
      "215 518\n",
      "216 518\n",
      "217 518\n",
      "218 518\n",
      "219 518\n",
      "220 518\n",
      "221 518\n",
      "222 518\n",
      "223 518\n",
      "224 518\n",
      "225 518\n",
      "226 518\n",
      "227 518\n",
      "228 518\n",
      "229 518\n",
      "230 518\n",
      "231 518\n",
      "232 518\n",
      "233 518\n",
      "234 518\n",
      "235 518\n",
      "236 518\n",
      "237 518\n",
      "238 518\n",
      "239 518\n",
      "240 518\n",
      "241 518\n",
      "242 518\n",
      "243 518\n",
      "244 518\n",
      "245 518\n",
      "246 518\n",
      "247 518\n",
      "248 518\n",
      "249 518\n",
      "250 518\n",
      "251 518\n",
      "252 518\n",
      "253 518\n",
      "254 518\n",
      "255 518\n",
      "256 518\n",
      "257 518\n",
      "258 518\n",
      "259 518\n",
      "260 518\n",
      "261 518\n",
      "262 518\n",
      "263 518\n",
      "264 518\n",
      "265 518\n",
      "266 518\n",
      "267 518\n",
      "268 518\n",
      "269 518\n",
      "270 518\n",
      "271 518\n",
      "272 518\n",
      "273 518\n",
      "274 518\n",
      "275 518\n",
      "276 518\n",
      "277 518\n",
      "278 518\n",
      "279 518\n",
      "280 518\n",
      "281 518\n",
      "282 518\n",
      "283 518\n",
      "284 518\n",
      "285 518\n",
      "286 518\n",
      "287 518\n",
      "288 518\n",
      "289 518\n",
      "290 518\n",
      "291 518\n",
      "292 518\n",
      "293 518\n",
      "294 518\n",
      "295 518\n",
      "296 518\n",
      "297 518\n",
      "298 518\n",
      "299 518\n",
      "300 518\n",
      "301 518\n",
      "302 518\n",
      "303 518\n",
      "304 518\n",
      "305 518\n",
      "306 518\n",
      "307 518\n",
      "308 518\n",
      "309 518\n",
      "310 518\n",
      "311 518\n",
      "312 518\n",
      "313 518\n",
      "314 518\n",
      "315 518\n",
      "316 518\n",
      "317 518\n",
      "318 518\n",
      "319 518\n",
      "320 518\n",
      "321 518\n",
      "322 518\n",
      "323 518\n",
      "324 518\n",
      "325 518\n",
      "326 518\n",
      "327 518\n",
      "328 518\n",
      "329 518\n",
      "330 518\n",
      "331 518\n",
      "332 518\n",
      "333 518\n",
      "334 518\n",
      "335 518\n",
      "336 518\n",
      "337 518\n",
      "338 518\n",
      "339 518\n",
      "340 518\n",
      "341 518\n",
      "342 518\n",
      "343 518\n",
      "344 518\n",
      "345 518\n",
      "346 518\n",
      "347 518\n",
      "348 518\n",
      "349 518\n",
      "350 518\n",
      "351 518\n",
      "352 518\n",
      "353 518\n",
      "354 518\n",
      "355 518\n",
      "356 518\n",
      "357 518\n",
      "358 518\n",
      "359 518\n",
      "360 518\n",
      "361 518\n",
      "362 518\n",
      "363 518\n",
      "364 518\n",
      "365 518\n",
      "366 518\n",
      "367 518\n",
      "368 518\n",
      "369 518\n",
      "370 518\n",
      "371 518\n",
      "372 518\n",
      "373 518\n",
      "374 518\n",
      "375 518\n",
      "376 518\n",
      "377 518\n",
      "378 518\n",
      "379 518\n",
      "380 518\n",
      "381 518\n",
      "382 518\n",
      "383 518\n",
      "384 518\n",
      "385 518\n",
      "386 518\n",
      "387 518\n",
      "388 518\n",
      "389 518\n",
      "390 518\n",
      "391 518\n",
      "392 518\n",
      "393 518\n",
      "394 518\n",
      "395 518\n",
      "396 518\n",
      "397 518\n",
      "398 518\n",
      "399 518\n",
      "400 518\n",
      "401 518\n",
      "402 518\n",
      "403 518\n",
      "404 518\n",
      "405 518\n",
      "406 518\n",
      "407 518\n",
      "408 518\n",
      "409 518\n",
      "410 518\n",
      "411 518\n",
      "412 518\n",
      "413 518\n",
      "414 518\n",
      "415 518\n",
      "416 518\n",
      "417 518\n",
      "418 518\n",
      "419 518\n",
      "420 518\n",
      "421 518\n",
      "422 518\n",
      "423 518\n",
      "424 518\n",
      "425 518\n",
      "426 518\n",
      "427 518\n",
      "428 518\n",
      "429 518\n",
      "430 518\n",
      "431 518\n",
      "432 518\n",
      "433 518\n",
      "434 518\n",
      "435 518\n",
      "436 518\n",
      "437 518\n",
      "438 518\n",
      "439 518\n",
      "440 518\n",
      "441 518\n",
      "442 518\n",
      "443 518\n",
      "444 518\n",
      "445 518\n",
      "446 518\n",
      "447 518\n",
      "448 518\n",
      "449 518\n",
      "450 518\n",
      "451 518\n",
      "452 518\n",
      "453 518\n",
      "454 518\n",
      "455 518\n",
      "456 518\n",
      "457 518\n",
      "458 518\n",
      "459 518\n",
      "460 518\n",
      "461 518\n",
      "462 518\n",
      "463 518\n",
      "464 518\n",
      "465 518\n",
      "466 518\n",
      "467 518\n",
      "468 518\n",
      "469 518\n",
      "470 518\n",
      "471 518\n",
      "472 518\n",
      "473 518\n",
      "474 518\n",
      "475 518\n",
      "476 518\n",
      "477 518\n",
      "478 518\n",
      "479 518\n",
      "480 518\n",
      "481 518\n",
      "482 518\n",
      "483 518\n",
      "484 518\n",
      "485 518\n",
      "486 518\n",
      "487 518\n",
      "488 518\n",
      "489 518\n",
      "490 518\n",
      "491 518\n",
      "492 518\n",
      "493 518\n",
      "494 518\n",
      "495 518\n",
      "496 518\n",
      "497 518\n",
      "498 518\n",
      "499 518\n",
      "500 518\n",
      "501 518\n",
      "502 518\n",
      "503 518\n",
      "504 518\n",
      "505 518\n",
      "506 518\n",
      "507 518\n",
      "508 518\n",
      "509 518\n",
      "510 518\n",
      "511 518\n",
      "512 518\n",
      "513 518\n",
      "514 518\n",
      "515 518\n",
      "516 518\n",
      "517 518\n",
      "Epoch 2: train_loss: 0.3592 train_acc: 0.8367 | val_loss: 0.4867 val_acc: 0.7891\n",
      "00:06:33.23\n",
      "0 518\n",
      "1 518\n",
      "2 518\n",
      "3 518\n",
      "4 518\n",
      "5 518\n",
      "6 518\n",
      "7 518\n",
      "8 518\n",
      "9 518\n",
      "10 518\n",
      "11 518\n",
      "12 518\n",
      "13 518\n",
      "14 518\n",
      "15 518\n",
      "16 518\n",
      "17 518\n",
      "18 518\n",
      "19 518\n",
      "20 518\n",
      "21 518\n",
      "22 518\n",
      "23 518\n",
      "24 518\n",
      "25 518\n",
      "26 518\n",
      "27 518\n",
      "28 518\n",
      "29 518\n",
      "30 518\n",
      "31 518\n",
      "32 518\n",
      "33 518\n",
      "34 518\n",
      "35 518\n",
      "36 518\n",
      "37 518\n",
      "38 518\n",
      "39 518\n",
      "40 518\n",
      "41 518\n",
      "42 518\n",
      "43 518\n",
      "44 518\n",
      "45 518\n",
      "46 518\n",
      "47 518\n",
      "48 518\n",
      "49 518\n",
      "50 518\n",
      "51 518\n",
      "52 518\n",
      "53 518\n",
      "54 518\n",
      "55 518\n",
      "56 518\n",
      "57 518\n",
      "58 518\n",
      "59 518\n",
      "60 518\n",
      "61 518\n",
      "62 518\n",
      "63 518\n",
      "64 518\n",
      "65 518\n",
      "66 518\n",
      "67 518\n",
      "68 518\n",
      "69 518\n",
      "70 518\n",
      "71 518\n",
      "72 518\n",
      "73 518\n",
      "74 518\n",
      "75 518\n",
      "76 518\n",
      "77 518\n",
      "78 518\n",
      "79 518\n",
      "80 518\n",
      "81 518\n",
      "82 518\n",
      "83 518\n",
      "84 518\n",
      "85 518\n",
      "86 518\n",
      "87 518\n",
      "88 518\n",
      "89 518\n",
      "90 518\n",
      "91 518\n",
      "92 518\n",
      "93 518\n",
      "94 518\n",
      "95 518\n",
      "96 518\n",
      "97 518\n",
      "98 518\n",
      "99 518\n",
      "100 518\n",
      "101 518\n",
      "102 518\n",
      "103 518\n",
      "104 518\n",
      "105 518\n",
      "106 518\n",
      "107 518\n",
      "108 518\n",
      "109 518\n",
      "110 518\n",
      "111 518\n",
      "112 518\n",
      "113 518\n",
      "114 518\n",
      "115 518\n",
      "116 518\n",
      "117 518\n",
      "118 518\n",
      "119 518\n",
      "120 518\n",
      "121 518\n",
      "122 518\n",
      "123 518\n",
      "124 518\n",
      "125 518\n",
      "126 518\n",
      "127 518\n",
      "128 518\n",
      "129 518\n",
      "130 518\n",
      "131 518\n",
      "132 518\n",
      "133 518\n",
      "134 518\n",
      "135 518\n",
      "136 518\n",
      "137 518\n",
      "138 518\n",
      "139 518\n",
      "140 518\n",
      "141 518\n",
      "142 518\n",
      "143 518\n",
      "144 518\n",
      "145 518\n",
      "146 518\n",
      "147 518\n",
      "148 518\n",
      "149 518\n",
      "150 518\n",
      "151 518\n",
      "152 518\n",
      "153 518\n",
      "154 518\n",
      "155 518\n",
      "156 518\n",
      "157 518\n",
      "158 518\n",
      "159 518\n",
      "160 518\n",
      "161 518\n",
      "162 518\n",
      "163 518\n",
      "164 518\n",
      "165 518\n",
      "166 518\n",
      "167 518\n",
      "168 518\n",
      "169 518\n",
      "170 518\n",
      "171 518\n",
      "172 518\n",
      "173 518\n",
      "174 518\n",
      "175 518\n",
      "176 518\n",
      "177 518\n",
      "178 518\n",
      "179 518\n",
      "180 518\n",
      "181 518\n",
      "182 518\n",
      "183 518\n",
      "184 518\n",
      "185 518\n",
      "186 518\n",
      "187 518\n",
      "188 518\n",
      "189 518\n",
      "190 518\n",
      "191 518\n",
      "192 518\n",
      "193 518\n",
      "194 518\n",
      "195 518\n",
      "196 518\n",
      "197 518\n",
      "198 518\n",
      "199 518\n",
      "200 518\n",
      "201 518\n",
      "202 518\n",
      "203 518\n",
      "204 518\n",
      "205 518\n",
      "206 518\n",
      "207 518\n",
      "208 518\n",
      "209 518\n",
      "210 518\n",
      "211 518\n",
      "212 518\n",
      "213 518\n",
      "214 518\n",
      "215 518\n",
      "216 518\n",
      "217 518\n",
      "218 518\n",
      "219 518\n",
      "220 518\n",
      "221 518\n",
      "222 518\n",
      "223 518\n",
      "224 518\n",
      "225 518\n",
      "226 518\n",
      "227 518\n",
      "228 518\n",
      "229 518\n",
      "230 518\n",
      "231 518\n",
      "232 518\n",
      "233 518\n",
      "234 518\n",
      "235 518\n",
      "236 518\n",
      "237 518\n",
      "238 518\n",
      "239 518\n",
      "240 518\n",
      "241 518\n",
      "242 518\n",
      "243 518\n",
      "244 518\n",
      "245 518\n",
      "246 518\n",
      "247 518\n",
      "248 518\n",
      "249 518\n",
      "250 518\n",
      "251 518\n",
      "252 518\n",
      "253 518\n",
      "254 518\n",
      "255 518\n",
      "256 518\n",
      "257 518\n",
      "258 518\n",
      "259 518\n",
      "260 518\n",
      "261 518\n",
      "262 518\n",
      "263 518\n",
      "264 518\n",
      "265 518\n",
      "266 518\n",
      "267 518\n",
      "268 518\n",
      "269 518\n",
      "270 518\n",
      "271 518\n",
      "272 518\n",
      "273 518\n",
      "274 518\n",
      "275 518\n",
      "276 518\n",
      "277 518\n",
      "278 518\n",
      "279 518\n",
      "280 518\n",
      "281 518\n",
      "282 518\n",
      "283 518\n",
      "284 518\n",
      "285 518\n",
      "286 518\n",
      "287 518\n",
      "288 518\n",
      "289 518\n",
      "290 518\n",
      "291 518\n",
      "292 518\n",
      "293 518\n",
      "294 518\n",
      "295 518\n",
      "296 518\n",
      "297 518\n",
      "298 518\n",
      "299 518\n",
      "300 518\n",
      "301 518\n",
      "302 518\n",
      "303 518\n",
      "304 518\n",
      "305 518\n",
      "306 518\n",
      "307 518\n",
      "308 518\n",
      "309 518\n",
      "310 518\n",
      "311 518\n",
      "312 518\n",
      "313 518\n",
      "314 518\n",
      "315 518\n",
      "316 518\n",
      "317 518\n",
      "318 518\n",
      "319 518\n",
      "320 518\n",
      "321 518\n",
      "322 518\n",
      "323 518\n",
      "324 518\n",
      "325 518\n",
      "326 518\n",
      "327 518\n",
      "328 518\n",
      "329 518\n",
      "330 518\n",
      "331 518\n",
      "332 518\n",
      "333 518\n",
      "334 518\n",
      "335 518\n",
      "336 518\n",
      "337 518\n",
      "338 518\n",
      "339 518\n",
      "340 518\n",
      "341 518\n",
      "342 518\n",
      "343 518\n",
      "344 518\n",
      "345 518\n",
      "346 518\n",
      "347 518\n",
      "348 518\n",
      "349 518\n",
      "350 518\n",
      "351 518\n",
      "352 518\n",
      "353 518\n",
      "354 518\n",
      "355 518\n",
      "356 518\n",
      "357 518\n",
      "358 518\n",
      "359 518\n",
      "360 518\n",
      "361 518\n",
      "362 518\n",
      "363 518\n",
      "364 518\n",
      "365 518\n",
      "366 518\n",
      "367 518\n",
      "368 518\n",
      "369 518\n",
      "370 518\n",
      "371 518\n",
      "372 518\n",
      "373 518\n",
      "374 518\n",
      "375 518\n",
      "376 518\n",
      "377 518\n",
      "378 518\n",
      "379 518\n",
      "380 518\n",
      "381 518\n",
      "382 518\n",
      "383 518\n",
      "384 518\n",
      "385 518\n",
      "386 518\n",
      "387 518\n",
      "388 518\n",
      "389 518\n",
      "390 518\n",
      "391 518\n",
      "392 518\n",
      "393 518\n",
      "394 518\n",
      "395 518\n",
      "396 518\n",
      "397 518\n",
      "398 518\n",
      "399 518\n",
      "400 518\n",
      "401 518\n",
      "402 518\n",
      "403 518\n",
      "404 518\n",
      "405 518\n",
      "406 518\n",
      "407 518\n",
      "408 518\n",
      "409 518\n",
      "410 518\n",
      "411 518\n",
      "412 518\n",
      "413 518\n",
      "414 518\n",
      "415 518\n",
      "416 518\n",
      "417 518\n",
      "418 518\n",
      "419 518\n",
      "420 518\n",
      "421 518\n",
      "422 518\n",
      "423 518\n",
      "424 518\n",
      "425 518\n",
      "426 518\n",
      "427 518\n",
      "428 518\n",
      "429 518\n",
      "430 518\n",
      "431 518\n",
      "432 518\n",
      "433 518\n",
      "434 518\n",
      "435 518\n",
      "436 518\n",
      "437 518\n",
      "438 518\n",
      "439 518\n",
      "440 518\n",
      "441 518\n",
      "442 518\n",
      "443 518\n",
      "444 518\n",
      "445 518\n",
      "446 518\n",
      "447 518\n",
      "448 518\n",
      "449 518\n",
      "450 518\n",
      "451 518\n",
      "452 518\n",
      "453 518\n",
      "454 518\n",
      "455 518\n",
      "456 518\n",
      "457 518\n",
      "458 518\n",
      "459 518\n",
      "460 518\n",
      "461 518\n",
      "462 518\n",
      "463 518\n",
      "464 518\n",
      "465 518\n",
      "466 518\n",
      "467 518\n",
      "468 518\n",
      "469 518\n",
      "470 518\n",
      "471 518\n",
      "472 518\n",
      "473 518\n",
      "474 518\n",
      "475 518\n",
      "476 518\n",
      "477 518\n",
      "478 518\n",
      "479 518\n",
      "480 518\n",
      "481 518\n",
      "482 518\n",
      "483 518\n",
      "484 518\n",
      "485 518\n",
      "486 518\n",
      "487 518\n",
      "488 518\n",
      "489 518\n",
      "490 518\n",
      "491 518\n",
      "492 518\n",
      "493 518\n",
      "494 518\n",
      "495 518\n",
      "496 518\n",
      "497 518\n",
      "498 518\n",
      "499 518\n",
      "500 518\n",
      "501 518\n",
      "502 518\n",
      "503 518\n",
      "504 518\n",
      "505 518\n",
      "506 518\n",
      "507 518\n",
      "508 518\n",
      "509 518\n",
      "510 518\n",
      "511 518\n",
      "512 518\n",
      "513 518\n",
      "514 518\n",
      "515 518\n",
      "516 518\n",
      "517 518\n",
      "Epoch 3: train_loss: 0.2559 train_acc: 0.8936 | val_loss: 0.5570 val_acc: 0.7825\n",
      "00:06:32.97\n",
      "0 518\n",
      "1 518\n",
      "2 518\n",
      "3 518\n",
      "4 518\n",
      "5 518\n",
      "6 518\n",
      "7 518\n",
      "8 518\n",
      "9 518\n",
      "10 518\n",
      "11 518\n",
      "12 518\n",
      "13 518\n",
      "14 518\n",
      "15 518\n",
      "16 518\n",
      "17 518\n",
      "18 518\n",
      "19 518\n",
      "20 518\n",
      "21 518\n",
      "22 518\n",
      "23 518\n",
      "24 518\n",
      "25 518\n",
      "26 518\n",
      "27 518\n",
      "28 518\n",
      "29 518\n",
      "30 518\n",
      "31 518\n",
      "32 518\n",
      "33 518\n",
      "34 518\n",
      "35 518\n",
      "36 518\n",
      "37 518\n",
      "38 518\n",
      "39 518\n",
      "40 518\n",
      "41 518\n",
      "42 518\n",
      "43 518\n",
      "44 518\n",
      "45 518\n",
      "46 518\n",
      "47 518\n",
      "48 518\n",
      "49 518\n",
      "50 518\n",
      "51 518\n",
      "52 518\n",
      "53 518\n",
      "54 518\n",
      "55 518\n",
      "56 518\n",
      "57 518\n",
      "58 518\n",
      "59 518\n",
      "60 518\n",
      "61 518\n",
      "62 518\n",
      "63 518\n",
      "64 518\n",
      "65 518\n",
      "66 518\n",
      "67 518\n",
      "68 518\n",
      "69 518\n",
      "70 518\n",
      "71 518\n",
      "72 518\n",
      "73 518\n",
      "74 518\n",
      "75 518\n",
      "76 518\n",
      "77 518\n",
      "78 518\n",
      "79 518\n",
      "80 518\n",
      "81 518\n",
      "82 518\n",
      "83 518\n",
      "84 518\n",
      "85 518\n",
      "86 518\n",
      "87 518\n",
      "88 518\n",
      "89 518\n",
      "90 518\n",
      "91 518\n",
      "92 518\n",
      "93 518\n",
      "94 518\n",
      "95 518\n",
      "96 518\n",
      "97 518\n",
      "98 518\n",
      "99 518\n",
      "100 518\n",
      "101 518\n",
      "102 518\n",
      "103 518\n",
      "104 518\n",
      "105 518\n",
      "106 518\n",
      "107 518\n",
      "108 518\n",
      "109 518\n",
      "110 518\n",
      "111 518\n",
      "112 518\n",
      "113 518\n",
      "114 518\n",
      "115 518\n",
      "116 518\n",
      "117 518\n",
      "118 518\n",
      "119 518\n",
      "120 518\n",
      "121 518\n",
      "122 518\n",
      "123 518\n",
      "124 518\n",
      "125 518\n",
      "126 518\n",
      "127 518\n",
      "128 518\n",
      "129 518\n",
      "130 518\n",
      "131 518\n",
      "132 518\n",
      "133 518\n",
      "134 518\n",
      "135 518\n",
      "136 518\n",
      "137 518\n",
      "138 518\n",
      "139 518\n",
      "140 518\n",
      "141 518\n",
      "142 518\n",
      "143 518\n",
      "144 518\n",
      "145 518\n",
      "146 518\n",
      "147 518\n",
      "148 518\n",
      "149 518\n",
      "150 518\n",
      "151 518\n",
      "152 518\n",
      "153 518\n",
      "154 518\n",
      "155 518\n",
      "156 518\n",
      "157 518\n",
      "158 518\n",
      "159 518\n",
      "160 518\n",
      "161 518\n",
      "162 518\n",
      "163 518\n",
      "164 518\n",
      "165 518\n",
      "166 518\n",
      "167 518\n",
      "168 518\n",
      "169 518\n",
      "170 518\n",
      "171 518\n",
      "172 518\n",
      "173 518\n",
      "174 518\n",
      "175 518\n",
      "176 518\n",
      "177 518\n",
      "178 518\n",
      "179 518\n",
      "180 518\n",
      "181 518\n",
      "182 518\n",
      "183 518\n",
      "184 518\n",
      "185 518\n",
      "186 518\n",
      "187 518\n",
      "188 518\n",
      "189 518\n",
      "190 518\n",
      "191 518\n",
      "192 518\n",
      "193 518\n",
      "194 518\n",
      "195 518\n",
      "196 518\n",
      "197 518\n",
      "198 518\n",
      "199 518\n",
      "200 518\n",
      "201 518\n",
      "202 518\n",
      "203 518\n",
      "204 518\n",
      "205 518\n",
      "206 518\n",
      "207 518\n",
      "208 518\n",
      "209 518\n",
      "210 518\n",
      "211 518\n",
      "212 518\n",
      "213 518\n",
      "214 518\n",
      "215 518\n",
      "216 518\n",
      "217 518\n",
      "218 518\n",
      "219 518\n",
      "220 518\n",
      "221 518\n",
      "222 518\n",
      "223 518\n",
      "224 518\n",
      "225 518\n",
      "226 518\n",
      "227 518\n",
      "228 518\n",
      "229 518\n",
      "230 518\n",
      "231 518\n",
      "232 518\n",
      "233 518\n",
      "234 518\n",
      "235 518\n",
      "236 518\n",
      "237 518\n",
      "238 518\n",
      "239 518\n",
      "240 518\n",
      "241 518\n",
      "242 518\n",
      "243 518\n",
      "244 518\n",
      "245 518\n",
      "246 518\n",
      "247 518\n",
      "248 518\n",
      "249 518\n",
      "250 518\n",
      "251 518\n",
      "252 518\n",
      "253 518\n",
      "254 518\n",
      "255 518\n",
      "256 518\n",
      "257 518\n",
      "258 518\n",
      "259 518\n",
      "260 518\n",
      "261 518\n",
      "262 518\n",
      "263 518\n",
      "264 518\n",
      "265 518\n",
      "266 518\n",
      "267 518\n",
      "268 518\n",
      "269 518\n",
      "270 518\n",
      "271 518\n",
      "272 518\n",
      "273 518\n",
      "274 518\n",
      "275 518\n",
      "276 518\n",
      "277 518\n",
      "278 518\n",
      "279 518\n",
      "280 518\n",
      "281 518\n",
      "282 518\n",
      "283 518\n",
      "284 518\n",
      "285 518\n",
      "286 518\n",
      "287 518\n",
      "288 518\n",
      "289 518\n",
      "290 518\n",
      "291 518\n",
      "292 518\n",
      "293 518\n",
      "294 518\n",
      "295 518\n",
      "296 518\n",
      "297 518\n",
      "298 518\n",
      "299 518\n",
      "300 518\n",
      "301 518\n",
      "302 518\n",
      "303 518\n",
      "304 518\n",
      "305 518\n",
      "306 518\n",
      "307 518\n",
      "308 518\n",
      "309 518\n",
      "310 518\n",
      "311 518\n",
      "312 518\n",
      "313 518\n",
      "314 518\n",
      "315 518\n",
      "316 518\n",
      "317 518\n",
      "318 518\n",
      "319 518\n",
      "320 518\n",
      "321 518\n",
      "322 518\n",
      "323 518\n",
      "324 518\n",
      "325 518\n",
      "326 518\n",
      "327 518\n",
      "328 518\n",
      "329 518\n",
      "330 518\n",
      "331 518\n",
      "332 518\n",
      "333 518\n",
      "334 518\n",
      "335 518\n",
      "336 518\n",
      "337 518\n",
      "338 518\n",
      "339 518\n",
      "340 518\n",
      "341 518\n",
      "342 518\n",
      "343 518\n",
      "344 518\n",
      "345 518\n",
      "346 518\n",
      "347 518\n",
      "348 518\n",
      "349 518\n",
      "350 518\n",
      "351 518\n",
      "352 518\n",
      "353 518\n",
      "354 518\n",
      "355 518\n",
      "356 518\n",
      "357 518\n",
      "358 518\n",
      "359 518\n",
      "360 518\n",
      "361 518\n",
      "362 518\n",
      "363 518\n",
      "364 518\n",
      "365 518\n",
      "366 518\n",
      "367 518\n",
      "368 518\n",
      "369 518\n",
      "370 518\n",
      "371 518\n",
      "372 518\n",
      "373 518\n",
      "374 518\n",
      "375 518\n",
      "376 518\n",
      "377 518\n",
      "378 518\n",
      "379 518\n",
      "380 518\n",
      "381 518\n",
      "382 518\n",
      "383 518\n",
      "384 518\n",
      "385 518\n",
      "386 518\n",
      "387 518\n",
      "388 518\n",
      "389 518\n",
      "390 518\n",
      "391 518\n",
      "392 518\n",
      "393 518\n",
      "394 518\n",
      "395 518\n",
      "396 518\n",
      "397 518\n",
      "398 518\n",
      "399 518\n",
      "400 518\n",
      "401 518\n",
      "402 518\n",
      "403 518\n",
      "404 518\n",
      "405 518\n",
      "406 518\n",
      "407 518\n",
      "408 518\n",
      "409 518\n",
      "410 518\n",
      "411 518\n",
      "412 518\n",
      "413 518\n",
      "414 518\n",
      "415 518\n",
      "416 518\n",
      "417 518\n",
      "418 518\n",
      "419 518\n",
      "420 518\n",
      "421 518\n",
      "422 518\n",
      "423 518\n",
      "424 518\n",
      "425 518\n",
      "426 518\n",
      "427 518\n",
      "428 518\n",
      "429 518\n",
      "430 518\n",
      "431 518\n",
      "432 518\n",
      "433 518\n",
      "434 518\n",
      "435 518\n",
      "436 518\n",
      "437 518\n",
      "438 518\n",
      "439 518\n",
      "440 518\n",
      "441 518\n",
      "442 518\n",
      "443 518\n",
      "444 518\n",
      "445 518\n",
      "446 518\n",
      "447 518\n",
      "448 518\n",
      "449 518\n",
      "450 518\n",
      "451 518\n",
      "452 518\n",
      "453 518\n",
      "454 518\n",
      "455 518\n",
      "456 518\n",
      "457 518\n",
      "458 518\n",
      "459 518\n",
      "460 518\n",
      "461 518\n",
      "462 518\n",
      "463 518\n",
      "464 518\n",
      "465 518\n",
      "466 518\n",
      "467 518\n",
      "468 518\n",
      "469 518\n",
      "470 518\n",
      "471 518\n",
      "472 518\n",
      "473 518\n",
      "474 518\n",
      "475 518\n",
      "476 518\n",
      "477 518\n",
      "478 518\n",
      "479 518\n",
      "480 518\n",
      "481 518\n",
      "482 518\n",
      "483 518\n",
      "484 518\n",
      "485 518\n",
      "486 518\n",
      "487 518\n",
      "488 518\n",
      "489 518\n",
      "490 518\n",
      "491 518\n",
      "492 518\n",
      "493 518\n",
      "494 518\n",
      "495 518\n",
      "496 518\n",
      "497 518\n",
      "498 518\n",
      "499 518\n",
      "500 518\n",
      "501 518\n",
      "502 518\n",
      "503 518\n",
      "504 518\n",
      "505 518\n",
      "506 518\n",
      "507 518\n",
      "508 518\n",
      "509 518\n",
      "510 518\n",
      "511 518\n",
      "512 518\n",
      "513 518\n",
      "514 518\n",
      "515 518\n",
      "516 518\n",
      "517 518\n",
      "Epoch 4: train_loss: 0.1662 train_acc: 0.9356 | val_loss: 0.5824 val_acc: 0.7857\n",
      "00:06:32.99\n",
      "0 518\n",
      "1 518\n",
      "2 518\n",
      "3 518\n",
      "4 518\n",
      "5 518\n",
      "6 518\n",
      "7 518\n",
      "8 518\n",
      "9 518\n",
      "10 518\n",
      "11 518\n",
      "12 518\n",
      "13 518\n",
      "14 518\n",
      "15 518\n",
      "16 518\n",
      "17 518\n",
      "18 518\n",
      "19 518\n",
      "20 518\n",
      "21 518\n",
      "22 518\n",
      "23 518\n",
      "24 518\n",
      "25 518\n",
      "26 518\n",
      "27 518\n",
      "28 518\n",
      "29 518\n",
      "30 518\n",
      "31 518\n",
      "32 518\n",
      "33 518\n",
      "34 518\n",
      "35 518\n",
      "36 518\n",
      "37 518\n",
      "38 518\n",
      "39 518\n",
      "40 518\n",
      "41 518\n",
      "42 518\n",
      "43 518\n",
      "44 518\n",
      "45 518\n",
      "46 518\n",
      "47 518\n",
      "48 518\n",
      "49 518\n",
      "50 518\n",
      "51 518\n",
      "52 518\n",
      "53 518\n",
      "54 518\n",
      "55 518\n",
      "56 518\n",
      "57 518\n",
      "58 518\n",
      "59 518\n",
      "60 518\n",
      "61 518\n",
      "62 518\n",
      "63 518\n",
      "64 518\n",
      "65 518\n",
      "66 518\n",
      "67 518\n",
      "68 518\n",
      "69 518\n",
      "70 518\n",
      "71 518\n",
      "72 518\n",
      "73 518\n",
      "74 518\n",
      "75 518\n",
      "76 518\n",
      "77 518\n",
      "78 518\n",
      "79 518\n",
      "80 518\n",
      "81 518\n",
      "82 518\n",
      "83 518\n",
      "84 518\n",
      "85 518\n",
      "86 518\n",
      "87 518\n",
      "88 518\n",
      "89 518\n",
      "90 518\n",
      "91 518\n",
      "92 518\n",
      "93 518\n",
      "94 518\n",
      "95 518\n",
      "96 518\n",
      "97 518\n",
      "98 518\n",
      "99 518\n",
      "100 518\n",
      "101 518\n",
      "102 518\n",
      "103 518\n",
      "104 518\n",
      "105 518\n",
      "106 518\n",
      "107 518\n",
      "108 518\n",
      "109 518\n",
      "110 518\n",
      "111 518\n",
      "112 518\n",
      "113 518\n",
      "114 518\n",
      "115 518\n",
      "116 518\n",
      "117 518\n",
      "118 518\n",
      "119 518\n",
      "120 518\n",
      "121 518\n",
      "122 518\n",
      "123 518\n",
      "124 518\n",
      "125 518\n",
      "126 518\n",
      "127 518\n",
      "128 518\n",
      "129 518\n",
      "130 518\n",
      "131 518\n",
      "132 518\n",
      "133 518\n",
      "134 518\n",
      "135 518\n",
      "136 518\n",
      "137 518\n",
      "138 518\n",
      "139 518\n",
      "140 518\n",
      "141 518\n",
      "142 518\n",
      "143 518\n",
      "144 518\n",
      "145 518\n",
      "146 518\n",
      "147 518\n",
      "148 518\n",
      "149 518\n",
      "150 518\n",
      "151 518\n",
      "152 518\n",
      "153 518\n",
      "154 518\n",
      "155 518\n",
      "156 518\n",
      "157 518\n",
      "158 518\n",
      "159 518\n",
      "160 518\n",
      "161 518\n",
      "162 518\n",
      "163 518\n",
      "164 518\n",
      "165 518\n",
      "166 518\n",
      "167 518\n",
      "168 518\n",
      "169 518\n",
      "170 518\n",
      "171 518\n",
      "172 518\n",
      "173 518\n",
      "174 518\n",
      "175 518\n",
      "176 518\n",
      "177 518\n",
      "178 518\n",
      "179 518\n",
      "180 518\n",
      "181 518\n",
      "182 518\n",
      "183 518\n",
      "184 518\n",
      "185 518\n",
      "186 518\n",
      "187 518\n",
      "188 518\n",
      "189 518\n",
      "190 518\n",
      "191 518\n",
      "192 518\n",
      "193 518\n",
      "194 518\n",
      "195 518\n",
      "196 518\n",
      "197 518\n",
      "198 518\n",
      "199 518\n",
      "200 518\n",
      "201 518\n",
      "202 518\n",
      "203 518\n",
      "204 518\n",
      "205 518\n",
      "206 518\n",
      "207 518\n",
      "208 518\n",
      "209 518\n",
      "210 518\n",
      "211 518\n",
      "212 518\n",
      "213 518\n",
      "214 518\n",
      "215 518\n",
      "216 518\n",
      "217 518\n",
      "218 518\n",
      "219 518\n",
      "220 518\n",
      "221 518\n",
      "222 518\n",
      "223 518\n",
      "224 518\n",
      "225 518\n",
      "226 518\n",
      "227 518\n",
      "228 518\n",
      "229 518\n",
      "230 518\n",
      "231 518\n",
      "232 518\n",
      "233 518\n",
      "234 518\n",
      "235 518\n",
      "236 518\n",
      "237 518\n",
      "238 518\n",
      "239 518\n",
      "240 518\n",
      "241 518\n",
      "242 518\n",
      "243 518\n",
      "244 518\n",
      "245 518\n",
      "246 518\n",
      "247 518\n",
      "248 518\n",
      "249 518\n",
      "250 518\n",
      "251 518\n",
      "252 518\n",
      "253 518\n",
      "254 518\n",
      "255 518\n",
      "256 518\n",
      "257 518\n",
      "258 518\n",
      "259 518\n",
      "260 518\n",
      "261 518\n",
      "262 518\n",
      "263 518\n",
      "264 518\n",
      "265 518\n",
      "266 518\n",
      "267 518\n",
      "268 518\n",
      "269 518\n",
      "270 518\n",
      "271 518\n",
      "272 518\n",
      "273 518\n",
      "274 518\n",
      "275 518\n",
      "276 518\n",
      "277 518\n",
      "278 518\n",
      "279 518\n",
      "280 518\n",
      "281 518\n",
      "282 518\n",
      "283 518\n",
      "284 518\n",
      "285 518\n",
      "286 518\n",
      "287 518\n",
      "288 518\n",
      "289 518\n",
      "290 518\n",
      "291 518\n",
      "292 518\n",
      "293 518\n",
      "294 518\n",
      "295 518\n",
      "296 518\n",
      "297 518\n",
      "298 518\n",
      "299 518\n",
      "300 518\n",
      "301 518\n",
      "302 518\n",
      "303 518\n",
      "304 518\n",
      "305 518\n",
      "306 518\n",
      "307 518\n",
      "308 518\n",
      "309 518\n",
      "310 518\n",
      "311 518\n",
      "312 518\n",
      "313 518\n",
      "314 518\n",
      "315 518\n",
      "316 518\n",
      "317 518\n",
      "318 518\n",
      "319 518\n",
      "320 518\n",
      "321 518\n",
      "322 518\n",
      "323 518\n",
      "324 518\n",
      "325 518\n",
      "326 518\n",
      "327 518\n",
      "328 518\n",
      "329 518\n",
      "330 518\n",
      "331 518\n",
      "332 518\n",
      "333 518\n",
      "334 518\n",
      "335 518\n",
      "336 518\n",
      "337 518\n",
      "338 518\n",
      "339 518\n",
      "340 518\n",
      "341 518\n",
      "342 518\n",
      "343 518\n",
      "344 518\n",
      "345 518\n",
      "346 518\n",
      "347 518\n",
      "348 518\n",
      "349 518\n",
      "350 518\n",
      "351 518\n",
      "352 518\n",
      "353 518\n",
      "354 518\n",
      "355 518\n",
      "356 518\n",
      "357 518\n",
      "358 518\n",
      "359 518\n",
      "360 518\n",
      "361 518\n",
      "362 518\n",
      "363 518\n",
      "364 518\n",
      "365 518\n",
      "366 518\n",
      "367 518\n",
      "368 518\n",
      "369 518\n",
      "370 518\n",
      "371 518\n",
      "372 518\n",
      "373 518\n",
      "374 518\n",
      "375 518\n",
      "376 518\n",
      "377 518\n",
      "378 518\n",
      "379 518\n",
      "380 518\n",
      "381 518\n",
      "382 518\n",
      "383 518\n",
      "384 518\n",
      "385 518\n",
      "386 518\n",
      "387 518\n",
      "388 518\n",
      "389 518\n",
      "390 518\n",
      "391 518\n",
      "392 518\n",
      "393 518\n",
      "394 518\n",
      "395 518\n",
      "396 518\n",
      "397 518\n",
      "398 518\n",
      "399 518\n",
      "400 518\n",
      "401 518\n",
      "402 518\n",
      "403 518\n",
      "404 518\n",
      "405 518\n",
      "406 518\n",
      "407 518\n",
      "408 518\n",
      "409 518\n",
      "410 518\n",
      "411 518\n",
      "412 518\n",
      "413 518\n",
      "414 518\n",
      "415 518\n",
      "416 518\n",
      "417 518\n",
      "418 518\n",
      "419 518\n",
      "420 518\n",
      "421 518\n",
      "422 518\n",
      "423 518\n",
      "424 518\n",
      "425 518\n",
      "426 518\n",
      "427 518\n",
      "428 518\n",
      "429 518\n",
      "430 518\n",
      "431 518\n",
      "432 518\n",
      "433 518\n",
      "434 518\n",
      "435 518\n",
      "436 518\n",
      "437 518\n",
      "438 518\n",
      "439 518\n",
      "440 518\n",
      "441 518\n",
      "442 518\n",
      "443 518\n",
      "444 518\n",
      "445 518\n",
      "446 518\n",
      "447 518\n",
      "448 518\n",
      "449 518\n",
      "450 518\n",
      "451 518\n",
      "452 518\n",
      "453 518\n",
      "454 518\n",
      "455 518\n",
      "456 518\n",
      "457 518\n",
      "458 518\n",
      "459 518\n",
      "460 518\n",
      "461 518\n",
      "462 518\n",
      "463 518\n",
      "464 518\n",
      "465 518\n",
      "466 518\n",
      "467 518\n",
      "468 518\n",
      "469 518\n",
      "470 518\n",
      "471 518\n",
      "472 518\n",
      "473 518\n",
      "474 518\n",
      "475 518\n",
      "476 518\n",
      "477 518\n",
      "478 518\n",
      "479 518\n",
      "480 518\n",
      "481 518\n",
      "482 518\n",
      "483 518\n",
      "484 518\n",
      "485 518\n",
      "486 518\n",
      "487 518\n",
      "488 518\n",
      "489 518\n",
      "490 518\n",
      "491 518\n",
      "492 518\n",
      "493 518\n",
      "494 518\n",
      "495 518\n",
      "496 518\n",
      "497 518\n",
      "498 518\n",
      "499 518\n",
      "500 518\n",
      "501 518\n",
      "502 518\n",
      "503 518\n",
      "504 518\n",
      "505 518\n",
      "506 518\n",
      "507 518\n",
      "508 518\n",
      "509 518\n",
      "510 518\n",
      "511 518\n",
      "512 518\n",
      "513 518\n",
      "514 518\n",
      "515 518\n",
      "516 518\n",
      "517 518\n",
      "Epoch 5: train_loss: 0.1003 train_acc: 0.9651 | val_loss: 0.6840 val_acc: 0.7782\n",
      "00:06:33.08\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "3nGBLgp312AD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.1076 test_acc: 0.6382\n",
      "00:01:22.70\n"
     ]
    }
   ],
   "source": [
    "preds, acts = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test accuracy is 0.6375\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(len(preds)):\n",
    "    acc += 1 if preds[i] == acts[i] else 0\n",
    "print(\"Overall test accuracy is\", acc/len(acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5w11ZMTvp6yr",
    "outputId": "879fa89b-bfca-4b6e-a749-d77eb8239ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.6371 test_acc: 0.5151\n",
      "00:00:45.00\n"
     ]
    }
   ],
   "source": [
    "preds_a, acts_a = test(model, test_loader_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hate examples: 1260\n",
      "Correctly predicted hate examples: 1204\n",
      "Accuracy for predicting hate examples: 0.9555555555555556\n",
      "Total non hate examples: 1740\n",
      "Correctly predicted non hate examples: 340\n",
      "Accuracy for predicting non hate examples: 0.19540229885057472\n"
     ]
    }
   ],
   "source": [
    "ent_acts_a = 0\n",
    "cont_acts_a = 0\n",
    "correct_ents_a = 0\n",
    "correct_conts_a = 0\n",
    "for i in range(len(preds_a)):\n",
    "    if acts_a[i] == 0:\n",
    "        ent_acts_a += 1\n",
    "        if preds_a[i] == 0:\n",
    "            correct_ents_a += 1\n",
    "    else:\n",
    "        cont_acts_a += 1\n",
    "        if preds_a[i] == 1:\n",
    "            correct_conts_a += 1\n",
    "            \n",
    "print(\"Total hate examples:\", ent_acts_a)\n",
    "print(\"Correctly predicted hate examples:\", correct_ents_a)\n",
    "print(\"Accuracy for predicting hate examples:\", correct_ents_a/ent_acts_a)\n",
    "print(\"Total non hate examples:\", cont_acts_a)\n",
    "print(\"Correctly predicted non hate examples:\", correct_conts_a)\n",
    "print(\"Accuracy for predicting non hate examples:\", correct_conts_a/cont_acts_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "uqIU3lTPqLkV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2630 test_acc: 0.8948\n",
      "00:00:08.68\n"
     ]
    }
   ],
   "source": [
    "preds_b, acts_b = test(model, test_loader_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total targeted hate examples: 529\n",
      "Correctly predicted targeted hate examples: 454\n",
      "Accuracy for predicting targeted hate examples: 0.8582230623818525\n",
      "Total non targeted offensive examples: 731\n",
      "Correctly predicted non targeted hate examples: 673\n",
      "Accuracy for predicting non targeted hate examples: 0.920656634746922\n"
     ]
    }
   ],
   "source": [
    "ent_acts_b = 0\n",
    "cont_acts_b = 0\n",
    "correct_ents_b = 0\n",
    "correct_conts_b = 0\n",
    "for i in range(len(preds_b)):\n",
    "    if acts_b[i] == 0:\n",
    "        ent_acts_b += 1\n",
    "        if preds_b[i] == 0:\n",
    "            correct_ents_b += 1\n",
    "    else:\n",
    "        cont_acts_b += 1\n",
    "        if preds_b[i] == 1:\n",
    "            correct_conts_b += 1\n",
    "            \n",
    "print(\"Total targeted hate examples:\", ent_acts_b)\n",
    "print(\"Correctly predicted targeted hate examples:\", correct_ents_b)\n",
    "print(\"Accuracy for predicting targeted hate examples:\", correct_ents_b/ent_acts_b)\n",
    "print(\"Total non targeted offensive examples:\", cont_acts_b)\n",
    "print(\"Correctly predicted non targeted hate examples:\", correct_conts_b)\n",
    "print(\"Accuracy for predicting non targeted hate examples:\", correct_conts_b/cont_acts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TyO6pUpqk1i",
    "outputId": "a1fa4716-4858-4ede-b8f5-89828ca0d40c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.6963 test_acc: 0.6729\n",
      "00:00:08.68\n"
     ]
    }
   ],
   "source": [
    "preds_c, acts_c = test(model, test_loader_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total aggressive hate examples: 594\n",
      "Correctly predicted aggressive hate examples: 402\n",
      "Accuracy for predicting aggressive hate examples: 0.6767676767676768\n",
      "Total non aggressive examples: 666\n",
      "Correctly non aggressive examples: 446\n",
      "Accuracy for predicting non aggressive examples: 0.6696696696696697\n"
     ]
    }
   ],
   "source": [
    "ent_acts_c = 0\n",
    "cont_acts_c = 0\n",
    "correct_ents_c = 0\n",
    "correct_conts_c = 0\n",
    "for i in range(len(preds_c)):\n",
    "    if acts_c[i] == 0:\n",
    "        ent_acts_c += 1\n",
    "        if preds_c[i] == 0:\n",
    "            correct_ents_c += 1\n",
    "    else:\n",
    "        cont_acts_c += 1\n",
    "        if preds_c[i] == 1:\n",
    "            correct_conts_c += 1\n",
    "            \n",
    "print(\"Total aggressive hate examples:\", ent_acts_c)\n",
    "print(\"Correctly predicted aggressive hate examples:\", correct_ents_c)\n",
    "print(\"Accuracy for predicting aggressive hate examples:\", correct_ents_c/ent_acts_c)\n",
    "print(\"Total non aggressive examples:\", cont_acts_c)\n",
    "print(\"Correctly non aggressive examples:\", correct_conts_c)\n",
    "print(\"Accuracy for predicting non aggressive examples:\", correct_conts_c/cont_acts_c)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentence Entailment BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0183e10141f049a1820827c9dfbd6c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d01f56954ef74452b649def7b9adc937",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a4784a513b749d68959b4250a8a1593",
      "value": 28
     }
    },
    "0504a8ce7be543a49611a424fcfc9ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1543919f31dd46c6a967a412af4d21bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16b6fa8b34e24eedb7c6c80e9e57b5d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24f1399337dc4cf998e189d697b8e282": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e8aaf8e2a644b8384d07e86578e7624": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30c1af9f395f4f148a4038c1f13b2088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e7c6e812fdc4b36a25096e9f176f75a",
       "IPY_MODEL_bf7f2daca2264bd289670898d7ab10e4",
       "IPY_MODEL_3af999385a094ccc85eadf702ab69723"
      ],
      "layout": "IPY_MODEL_1543919f31dd46c6a967a412af4d21bd"
     }
    },
    "322cc0d27aea4458a9efe1c099943482": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0d781c1aae342ae949264fb561b7f5f",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_677cf9fc8f3d47c68e73bd4a3622132b",
      "value": 466062
     }
    },
    "373af73466be45f1bb216e6abbdb15be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3af999385a094ccc85eadf702ab69723": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e50074e3a054225842a6fe8ef936183",
      "placeholder": "​",
      "style": "IPY_MODEL_c2496080cc384158856247dd5581e751",
      "value": " 570/570 [00:00&lt;00:00, 13.4kB/s]"
     }
    },
    "3e50074e3a054225842a6fe8ef936183": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "405ea21b2692487298e89e719c6573b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd90fde29e26447c9eaeef9aaf15d01f",
      "placeholder": "​",
      "style": "IPY_MODEL_16b6fa8b34e24eedb7c6c80e9e57b5d2",
      "value": "Downloading: 100%"
     }
    },
    "41fe1073e1ea44669cdbef98949f9528": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43f942f812884ace90aa9b1dd41a3d9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45dcdc23c9b4481a9e40878e4dbfd585": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2a5689111c3420c9ed4114bb55cf0fc",
      "placeholder": "​",
      "style": "IPY_MODEL_d09c0639c5ef45a7823074eb648de6d7",
      "value": "Downloading: 100%"
     }
    },
    "4720c540eecc4dbe8fee8c9596f56933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "546361d1061243f5b7fc5692b1c9d4c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d80c6db4dfc4bf0aaea82350dffdc21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "602d9b5979084873996eab9784b98edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6497737147ce458b881bbb1b17492364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "677cf9fc8f3d47c68e73bd4a3622132b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b127ca5fb5a4d23b6c3ffe654cdfcaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c0f5454779b4baeafa828e961459116": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a39de3d6df814e98be1a65e049bff39e",
       "IPY_MODEL_ef706620a1e34c3a84de18a03a85f8cd",
       "IPY_MODEL_bfee36d5b72d469cb308f4007e1474a0"
      ],
      "layout": "IPY_MODEL_f17b3761a665477e8fdecfd23d267f87"
     }
    },
    "6f61782328964e4dbdea05bee3797ee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb792d8fbadc4bb6977a1986a9783e15",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b127ca5fb5a4d23b6c3ffe654cdfcaf",
      "value": 440473133
     }
    },
    "7e7c6e812fdc4b36a25096e9f176f75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e8aaf8e2a644b8384d07e86578e7624",
      "placeholder": "​",
      "style": "IPY_MODEL_d84069b7b1384beaa5100505f9233b2b",
      "value": "Downloading: 100%"
     }
    },
    "814534237a9144deb7d52e4ceffabc43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6497737147ce458b881bbb1b17492364",
      "placeholder": "​",
      "style": "IPY_MODEL_373af73466be45f1bb216e6abbdb15be",
      "value": " 420M/420M [00:10&lt;00:00, 36.1MB/s]"
     }
    },
    "8665ae01129141dfb7fdf5037a994c86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93f0f8c7257944e6ab97295f09821f1d",
      "placeholder": "​",
      "style": "IPY_MODEL_a61965e1ea50438c868778deb83db312",
      "value": " 28.0/28.0 [00:00&lt;00:00, 518B/s]"
     }
    },
    "89a7fc4f932041a7b76b2502096ede49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a4784a513b749d68959b4250a8a1593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93ded7574b56441eaa9b6aaaa30840f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93f0f8c7257944e6ab97295f09821f1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95d0e5c35a01408fa9bde956d0785d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_405ea21b2692487298e89e719c6573b0",
       "IPY_MODEL_0183e10141f049a1820827c9dfbd6c5a",
       "IPY_MODEL_8665ae01129141dfb7fdf5037a994c86"
      ],
      "layout": "IPY_MODEL_93ded7574b56441eaa9b6aaaa30840f3"
     }
    },
    "9d383d03f9304bce914326ea0e5d472b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a39de3d6df814e98be1a65e049bff39e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24f1399337dc4cf998e189d697b8e282",
      "placeholder": "​",
      "style": "IPY_MODEL_f3dea62699df437c945160dfc42afbd5",
      "value": "Downloading: 100%"
     }
    },
    "a61965e1ea50438c868778deb83db312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf065a9136dc4262a74c04f746b7c143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d383d03f9304bce914326ea0e5d472b",
      "placeholder": "​",
      "style": "IPY_MODEL_43f942f812884ace90aa9b1dd41a3d9a",
      "value": "Downloading: 100%"
     }
    },
    "bf7f2daca2264bd289670898d7ab10e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d80c6db4dfc4bf0aaea82350dffdc21",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89a7fc4f932041a7b76b2502096ede49",
      "value": 570
     }
    },
    "bfee36d5b72d469cb308f4007e1474a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcde074381814577a68122dd6e5adf30",
      "placeholder": "​",
      "style": "IPY_MODEL_4720c540eecc4dbe8fee8c9596f56933",
      "value": " 226k/226k [00:00&lt;00:00, 215kB/s]"
     }
    },
    "c2496080cc384158856247dd5581e751": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c38a9f17f85544b68ffdc8e501a95f20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45dcdc23c9b4481a9e40878e4dbfd585",
       "IPY_MODEL_322cc0d27aea4458a9efe1c099943482",
       "IPY_MODEL_e4669478d1474c4485c45907447d75a5"
      ],
      "layout": "IPY_MODEL_41fe1073e1ea44669cdbef98949f9528"
     }
    },
    "d01f56954ef74452b649def7b9adc937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d09c0639c5ef45a7823074eb648de6d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d84069b7b1384beaa5100505f9233b2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcde074381814577a68122dd6e5adf30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd90fde29e26447c9eaeef9aaf15d01f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0c07e3c6bd042c2a3dd2aed325f6759": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0d781c1aae342ae949264fb561b7f5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4669478d1474c4485c45907447d75a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_546361d1061243f5b7fc5692b1c9d4c5",
      "placeholder": "​",
      "style": "IPY_MODEL_602d9b5979084873996eab9784b98edc",
      "value": " 455k/455k [00:00&lt;00:00, 691kB/s]"
     }
    },
    "e8a66bb5d7614602ac93db9cac811135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf065a9136dc4262a74c04f746b7c143",
       "IPY_MODEL_6f61782328964e4dbdea05bee3797ee9",
       "IPY_MODEL_814534237a9144deb7d52e4ceffabc43"
      ],
      "layout": "IPY_MODEL_0504a8ce7be543a49611a424fcfc9ca7"
     }
    },
    "ef706620a1e34c3a84de18a03a85f8cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0c07e3c6bd042c2a3dd2aed325f6759",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff8facd357854c18976dea8f1621cae6",
      "value": 231508
     }
    },
    "f17b3761a665477e8fdecfd23d267f87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a5689111c3420c9ed4114bb55cf0fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3dea62699df437c945160dfc42afbd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb792d8fbadc4bb6977a1986a9783e15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff8facd357854c18976dea8f1621cae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
