{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25QT54O2UjEh"
   },
   "source": [
    "Reference: https://github.com/dh1105/Sentence-Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YGhQEYnj7DD",
    "outputId": "819c882f-b0a1-4459-c424-d46e5be61b4f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uqcn3_lQ5Hhr",
    "outputId": "480b8d47-49e8-407d-cbb0-de91efbd2a15"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "## !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JQ4M8Znd4_ep"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkpfPDS1Z8a8",
    "outputId": "f7f15058-4a5a-44b3-a978-36a6f499f710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-eyFJM1SstoH"
   },
   "outputs": [],
   "source": [
    "task_a_hyp = \"This is offensive speech.\"\n",
    "task_b_hyp = \"This is targeted offense.\"\n",
    "task_c_hyp_1= \"This is targeted towards an individual.\"\n",
    "task_c_hyp_2 = \"This is targeted towards a group.\"\n",
    "task_c_hyp_3 = \"This isn't targeted towards a group or an individual.\"\n",
    "# f = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/olid_train_v2.csv')\n",
    "# f_a = open('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/bert_nli.csv', 'w')\n",
    "f = open('Data/olid_train_v2.csv')\n",
    "f_a = open('Data/bert_nli.csv', 'w')\n",
    "f_a.write('tweet_id' + '\\t' + 'gold_label' + '\\t' + 'sentence1' + '\\t' + 'sentence2' + '\\n')\n",
    "lines = f.readlines()\n",
    "for line in lines[1:]:\n",
    "  row = line.split('\\t')\n",
    "  tweet_id = row[0].strip()\n",
    "  tweet_text = ' '.join(row[1:-3]).strip()\n",
    "  is_offensive = row[-3].strip()\n",
    "  is_targeted = row[-2].strip()\n",
    "  target = row[-1].strip()\n",
    "  if is_offensive == 'OFF':\n",
    "    f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')\n",
    "    if is_targeted == 'TIN':\n",
    "      f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "      if target == 'IND':\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_c_hyp_1 + '\\n')\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp_2 + '\\n')\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp_3 + '\\n')\n",
    "      elif target == 'GRP':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp_1 + '\\n')\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_c_hyp_2 + '\\n')\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp_3 + '\\n')\n",
    "      elif target == 'OTH':\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp_1 + '\\n')\n",
    "        f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_c_hyp_2 + '\\n')\n",
    "        f_a.write(tweet_id + '\\t' + 'entailment' + '\\t' + tweet_text + '\\t' + task_c_hyp_3 + '\\n')\n",
    "    elif is_targeted == 'UNT':\n",
    "      f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_b_hyp + '\\n')\n",
    "  else:\n",
    "    f_a.write(tweet_id + '\\t' + 'contradiction' + '\\t' + tweet_text + '\\t' + task_a_hyp + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DCcdjRLDG3Ex"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Data/bert_nli.csv', delimiter='\\t')\n",
    "df = pd.read_csv('./Data/bert_nli.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fbQWldbaIOGR"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FH6Mk5reBrpJ"
   },
   "outputs": [],
   "source": [
    "l = len(df)\n",
    "train_df = df[:int(0.8*l)]\n",
    "val_df = df[int(0.8*l):int(0.9*l)]\n",
    "test_df = df[int(0.9*l):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3NgPYXBg4_e7"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UMTQb1iMF54W"
   },
   "outputs": [],
   "source": [
    "train_df['sentence1'] = train_df['sentence1'].astype(str)\n",
    "train_df['sentence2'] = train_df['sentence2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GMzPNmtpF8yg"
   },
   "outputs": [],
   "source": [
    "val_df['sentence1'] = val_df['sentence1'].astype(str)\n",
    "val_df['sentence2'] = val_df['sentence2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6UBcmYsPzWA7"
   },
   "outputs": [],
   "source": [
    "test_df['sentence1'] = test_df['sentence1'].astype(str)\n",
    "test_df['sentence2'] = test_df['sentence2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "diS3wCm14_e9"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]\n",
    "val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]\n",
    "test_df = test_df[(test_df['sentence1'].str.split().str.len() > 0) & (test_df['sentence2'].str.split().str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WinUrXE04_e9",
    "outputId": "6e4a47fe-cad1-4b40-c83f-acef182072c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6264</th>\n",
       "      <td>18807</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER Also Puerto Rico is a island add in the ...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21153</th>\n",
       "      <td>31019</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER @USER Fuck you...  If she dare the...</td>\n",
       "      <td>This is targeted towards a group.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21449</th>\n",
       "      <td>87056</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER @USER No they aren’t! They have all said...</td>\n",
       "      <td>This is targeted towards a group.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27957</th>\n",
       "      <td>48800</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>1/2 URL So because he speaks in a manner that ...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>59998</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER You can probably imagine all the SJW sno...</td>\n",
       "      <td>This is targeted offense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25116</th>\n",
       "      <td>87588</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>This explains a lot: URL  #MAGA</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22871</th>\n",
       "      <td>13519</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER @USER @USER Don't you liberals wan...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26855</th>\n",
       "      <td>36284</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER We get shit transported from Africa nigg...</td>\n",
       "      <td>This is targeted towards an individual.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17086</th>\n",
       "      <td>95833</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER @USER Wow!  I like this lady more ...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22794</th>\n",
       "      <td>49109</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER Oh well. Its tje liberals and socialists...</td>\n",
       "      <td>This isn't targeted towards a group or an indi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22520 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id     gold_label  \\\n",
       "6264      18807  contradiction   \n",
       "21153     31019  contradiction   \n",
       "21449     87056     entailment   \n",
       "27957     48800  contradiction   \n",
       "277       59998     entailment   \n",
       "...         ...            ...   \n",
       "25116     87588  contradiction   \n",
       "22871     13519  contradiction   \n",
       "26855     36284  contradiction   \n",
       "17086     95833  contradiction   \n",
       "22794     49109  contradiction   \n",
       "\n",
       "                                               sentence1  \\\n",
       "6264   @USER Also Puerto Rico is a island add in the ...   \n",
       "21153  @USER @USER @USER Fuck you...  If she dare the...   \n",
       "21449  @USER @USER No they aren’t! They have all said...   \n",
       "27957  1/2 URL So because he speaks in a manner that ...   \n",
       "277    @USER You can probably imagine all the SJW sno...   \n",
       "...                                                  ...   \n",
       "25116                    This explains a lot: URL  #MAGA   \n",
       "22871  @USER @USER @USER @USER Don't you liberals wan...   \n",
       "26855  @USER We get shit transported from Africa nigg...   \n",
       "17086  @USER @USER @USER Wow!  I like this lady more ...   \n",
       "22794  @USER Oh well. Its tje liberals and socialists...   \n",
       "\n",
       "                                               sentence2  \n",
       "6264                           This is offensive speech.  \n",
       "21153                  This is targeted towards a group.  \n",
       "21449                  This is targeted towards a group.  \n",
       "27957                          This is offensive speech.  \n",
       "277                            This is targeted offense.  \n",
       "...                                                  ...  \n",
       "25116                          This is offensive speech.  \n",
       "22871                          This is offensive speech.  \n",
       "26855            This is targeted towards an individual.  \n",
       "17086                          This is offensive speech.  \n",
       "22794  This isn't targeted towards a group or an indi...  \n",
       "\n",
       "[22520 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6e4dxTks4_e-",
    "outputId": "94ec80ad-9e97-49bf-8b63-4fef22a25fa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14065</th>\n",
       "      <td>97581</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER you suck so fucking hard that i genuinel...</td>\n",
       "      <td>This is targeted offense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23666</th>\n",
       "      <td>60683</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER Wake up black Americans!  #maga #Kavanau...</td>\n",
       "      <td>This is targeted towards a group.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>12476</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER @USER Trump is declassifying infor...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>39490</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER @USER Whenever conservatives lose ...</td>\n",
       "      <td>This isn't targeted towards a group or an indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>55559</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>#TuesdayThought.   The only hate speech that I...</td>\n",
       "      <td>This is targeted towards a group.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>89799</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Impeach Trump and #MAGA</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17250</th>\n",
       "      <td>49204</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER AARON RODGERS WILL NEVER WIN A RING THE ...</td>\n",
       "      <td>This is targeted towards a group.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26653</th>\n",
       "      <td>96023</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER “has been the inspiration for the ...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>59858</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER @USER @USER @USER Chris Burns you are a ...</td>\n",
       "      <td>This is targeted offense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>56068</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER I love the way this pisses off liberals....</td>\n",
       "      <td>This isn't targeted towards a group or an indi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2815 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id     gold_label  \\\n",
       "14065     97581     entailment   \n",
       "23666     60683     entailment   \n",
       "17606     12476  contradiction   \n",
       "12118     39490  contradiction   \n",
       "4916      55559  contradiction   \n",
       "...         ...            ...   \n",
       "4381      89799     entailment   \n",
       "17250     49204  contradiction   \n",
       "26653     96023  contradiction   \n",
       "5526      59858     entailment   \n",
       "875       56068  contradiction   \n",
       "\n",
       "                                               sentence1  \\\n",
       "14065  @USER you suck so fucking hard that i genuinel...   \n",
       "23666  @USER Wake up black Americans!  #maga #Kavanau...   \n",
       "17606  @USER @USER @USER Trump is declassifying infor...   \n",
       "12118  @USER @USER @USER Whenever conservatives lose ...   \n",
       "4916   #TuesdayThought.   The only hate speech that I...   \n",
       "...                                                  ...   \n",
       "4381                             Impeach Trump and #MAGA   \n",
       "17250  @USER AARON RODGERS WILL NEVER WIN A RING THE ...   \n",
       "26653  @USER @USER “has been the inspiration for the ...   \n",
       "5526   @USER @USER @USER @USER Chris Burns you are a ...   \n",
       "875    @USER I love the way this pisses off liberals....   \n",
       "\n",
       "                                               sentence2  \n",
       "14065                          This is targeted offense.  \n",
       "23666                  This is targeted towards a group.  \n",
       "17606                          This is offensive speech.  \n",
       "12118  This isn't targeted towards a group or an indi...  \n",
       "4916                   This is targeted towards a group.  \n",
       "...                                                  ...  \n",
       "4381                           This is offensive speech.  \n",
       "17250                  This is targeted towards a group.  \n",
       "26653                          This is offensive speech.  \n",
       "5526                           This is targeted offense.  \n",
       "875    This isn't targeted towards a group or an indi...  \n",
       "\n",
       "[2815 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-nQRV3wZzlMe",
    "outputId": "601de039-4efc-4156-b7c6-364876406bba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>77105</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER in the fake national emergency that was ...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23144</th>\n",
       "      <td>46226</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER Who the hell does @USER think he is to s...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>17595</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER @USER @USER No. She is a liar.</td>\n",
       "      <td>This is targeted towards an individual.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>34887</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER @USER yeah keep mocking her right into a...</td>\n",
       "      <td>This is targeted offense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>23596</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER Fear about losing your monopoly status i...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>96139</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER You are TRIPPIN. Stop jumping to conclus...</td>\n",
       "      <td>This is targeted offense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13683</th>\n",
       "      <td>90804</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER HIRE AMERICAN YOU PIECES OF TRASH!...</td>\n",
       "      <td>This isn't targeted towards a group or an indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>30412</td>\n",
       "      <td>entailment</td>\n",
       "      <td>@USER How he is still batting</td>\n",
       "      <td>This is targeted offense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>52824</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER Good girl @USER</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8107</th>\n",
       "      <td>87192</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>@USER @USER Phil is right socialists are the n...</td>\n",
       "      <td>This is offensive speech.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2816 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id     gold_label  \\\n",
       "3092      77105     entailment   \n",
       "23144     46226  contradiction   \n",
       "5560      17595     entailment   \n",
       "5372      34887     entailment   \n",
       "10122     23596  contradiction   \n",
       "...         ...            ...   \n",
       "1763      96139     entailment   \n",
       "13683     90804  contradiction   \n",
       "2648      30412     entailment   \n",
       "255       52824  contradiction   \n",
       "8107      87192  contradiction   \n",
       "\n",
       "                                               sentence1  \\\n",
       "3092   @USER in the fake national emergency that was ...   \n",
       "23144  @USER Who the hell does @USER think he is to s...   \n",
       "5560                @USER @USER @USER No. She is a liar.   \n",
       "5372   @USER @USER yeah keep mocking her right into a...   \n",
       "10122  @USER Fear about losing your monopoly status i...   \n",
       "...                                                  ...   \n",
       "1763   @USER You are TRIPPIN. Stop jumping to conclus...   \n",
       "13683  @USER @USER HIRE AMERICAN YOU PIECES OF TRASH!...   \n",
       "2648                       @USER How he is still batting   \n",
       "255                                @USER Good girl @USER   \n",
       "8107   @USER @USER Phil is right socialists are the n...   \n",
       "\n",
       "                                               sentence2  \n",
       "3092                           This is offensive speech.  \n",
       "23144                          This is offensive speech.  \n",
       "5560             This is targeted towards an individual.  \n",
       "5372                           This is targeted offense.  \n",
       "10122                          This is offensive speech.  \n",
       "...                                                  ...  \n",
       "1763                           This is targeted offense.  \n",
       "13683  This isn't targeted towards a group or an indi...  \n",
       "2648                           This is targeted offense.  \n",
       "255                            This is offensive speech.  \n",
       "8107                           This is offensive speech.  \n",
       "\n",
       "[2816 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "P11Q5BRkX70D"
   },
   "outputs": [],
   "source": [
    "test_df_a = test_df[test_df.sentence2 == \"This is offensive speech.\"]\n",
    "test_df_b = test_df[test_df.sentence2 == \"This is targeted offense.\"]\n",
    "test_df_c = test_df[test_df.sentence2 == \"This is targeted towards an individual.\"]\n",
    "test_df_d = test_df[test_df.sentence2 == \"This is targeted towards a group.\"]\n",
    "test_df_e = test_df[test_df.sentence2 == \"This isn't targeted towards a group or an individual.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSjkMi9VZ4N2",
    "outputId": "5f824c54-3425-48d2-faa7-a7f85693565c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271\n",
      "436\n",
      "364\n",
      "362\n",
      "383\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df_a))\n",
    "print(len(test_df_b))\n",
    "print(len(test_df_c))\n",
    "print(len(test_df_d))\n",
    "print(len(test_df_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Gk96lNh94_e_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df, test_df_a, test_df_b, test_df_c, test_df_d, test_df_e):\n",
    "    self.label_dict = {'entailment': 0, 'contradiction': 1}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "    self.test_df_a = test_df_a\n",
    "    self.test_df_b = test_df_b\n",
    "    self.test_df_c = test_df_c\n",
    "    self.test_df_d = test_df_d\n",
    "    self.test_df_e = test_df_e\n",
    "    #self.base_path = '/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/mnli-data'\n",
    "    self.base_path = 'mnli-data'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.test_data_a = None\n",
    "    self.test_data_b = None\n",
    "    self.test_data_c = None\n",
    "    self.test_data_d = None\n",
    "    self.test_data_e = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    #Saving takes too much RAM\n",
    "    \n",
    "    if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\n",
    "      print(\"Found training data\")\n",
    "      with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\n",
    "        self.train_data = pickle.load(f)\n",
    "    else:\n",
    "      self.train_data = self.load_data(self.train_df)\n",
    "      with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.train_data, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\n",
    "      print(\"Found val data\")\n",
    "      with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\n",
    "        self.val_data = pickle.load(f)\n",
    "    else:\n",
    "      self.val_data = self.load_data(self.val_df)\n",
    "      with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.val_data, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data.pkl')):\n",
    "      print(\"Found test data\")\n",
    "      with open(os.path.join(self.base_path, 'test_data.pkl'), 'rb') as f:\n",
    "        self.test_data = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data = self.load_data(self.test_df)\n",
    "      with open(os.path.join(self.base_path, 'test_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_a.pkl')):\n",
    "      print(\"Found test data a\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_a.pkl'), 'rb') as f:\n",
    "        self.test_data_a = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_a = self.load_data(self.test_df_a)\n",
    "      with open(os.path.join(self.base_path, 'test_data_a.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_a, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_b.pkl')):\n",
    "      print(\"Found test data b\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_b.pkl'), 'rb') as f:\n",
    "        self.test_data_b = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_b = self.load_data(self.test_df_b)\n",
    "      with open(os.path.join(self.base_path, 'test_data_b.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_b, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_c.pkl')):\n",
    "      print(\"Found test data c\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_c.pkl'), 'rb') as f:\n",
    "        self.test_data_c = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_c = self.load_data(self.test_df_c)\n",
    "      with open(os.path.join(self.base_path, 'test_data_c.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_c, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_d.pkl')):\n",
    "      print(\"Found test data d\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_d.pkl'), 'rb') as f:\n",
    "        self.test_data_d = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_d = self.load_data(self.test_df_d)\n",
    "      with open(os.path.join(self.base_path, 'test_data_d.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_d, f)\n",
    "    if os.path.exists(os.path.join(self.base_path, 'test_data_e.pkl')):\n",
    "      print(\"Found test data e\")\n",
    "      with open(os.path.join(self.base_path, 'test_data_e.pkl'), 'rb') as f:\n",
    "        self.test_data_e = pickle.load(f)\n",
    "    else:\n",
    "      self.test_data_e = self.load_data(self.test_df_e)\n",
    "      with open(os.path.join(self.base_path, 'test_data_e.pkl'), 'wb') as f:\n",
    "        pickle.dump(self.test_data_e, f)\n",
    "    # self.train_data = self.load_data(self.train_df)\n",
    "    # self.val_data = self.load_data(self.val_df)\n",
    "    # self.test_data = self.load_data(self.test_df)\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['sentence1'].to_list()\n",
    "    hypothesis_list = df['sentence2'].to_list()\n",
    "    label_list = df['gold_label'].to_list()\n",
    "\n",
    "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_a = DataLoader(\n",
    "      self.test_data_a,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_b = DataLoader(\n",
    "      self.test_data_b,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_c = DataLoader(\n",
    "      self.test_data_c,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_d = DataLoader(\n",
    "      self.test_data_d,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loader_e = DataLoader(\n",
    "      self.test_data_e,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, test_loader_a, test_loader_b, test_loader_c, test_loader_d, test_loader_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "6c0f5454779b4baeafa828e961459116",
      "f17b3761a665477e8fdecfd23d267f87",
      "a39de3d6df814e98be1a65e049bff39e",
      "ef706620a1e34c3a84de18a03a85f8cd",
      "bfee36d5b72d469cb308f4007e1474a0",
      "f3dea62699df437c945160dfc42afbd5",
      "24f1399337dc4cf998e189d697b8e282",
      "ff8facd357854c18976dea8f1621cae6",
      "e0c07e3c6bd042c2a3dd2aed325f6759",
      "4720c540eecc4dbe8fee8c9596f56933",
      "dcde074381814577a68122dd6e5adf30",
      "95d0e5c35a01408fa9bde956d0785d21",
      "93ded7574b56441eaa9b6aaaa30840f3",
      "405ea21b2692487298e89e719c6573b0",
      "0183e10141f049a1820827c9dfbd6c5a",
      "8665ae01129141dfb7fdf5037a994c86",
      "16b6fa8b34e24eedb7c6c80e9e57b5d2",
      "dd90fde29e26447c9eaeef9aaf15d01f",
      "8a4784a513b749d68959b4250a8a1593",
      "d01f56954ef74452b649def7b9adc937",
      "a61965e1ea50438c868778deb83db312",
      "93f0f8c7257944e6ab97295f09821f1d",
      "c38a9f17f85544b68ffdc8e501a95f20",
      "41fe1073e1ea44669cdbef98949f9528",
      "45dcdc23c9b4481a9e40878e4dbfd585",
      "322cc0d27aea4458a9efe1c099943482",
      "e4669478d1474c4485c45907447d75a5",
      "d09c0639c5ef45a7823074eb648de6d7",
      "f2a5689111c3420c9ed4114bb55cf0fc",
      "677cf9fc8f3d47c68e73bd4a3622132b",
      "e0d781c1aae342ae949264fb561b7f5f",
      "602d9b5979084873996eab9784b98edc",
      "546361d1061243f5b7fc5692b1c9d4c5",
      "30c1af9f395f4f148a4038c1f13b2088",
      "1543919f31dd46c6a967a412af4d21bd",
      "7e7c6e812fdc4b36a25096e9f176f75a",
      "bf7f2daca2264bd289670898d7ab10e4",
      "3af999385a094ccc85eadf702ab69723",
      "d84069b7b1384beaa5100505f9233b2b",
      "2e8aaf8e2a644b8384d07e86578e7624",
      "89a7fc4f932041a7b76b2502096ede49",
      "5d80c6db4dfc4bf0aaea82350dffdc21",
      "c2496080cc384158856247dd5581e751",
      "3e50074e3a054225842a6fe8ef936183"
     ]
    },
    "id": "md52P1z14_e_",
    "outputId": "e875db6f-6bb9-44a1-f2de-0e8cbea3e883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found training data\n",
      "Found val data\n",
      "Found test data\n",
      "Found test data a\n",
      "Found test data b\n",
      "Found test data c\n",
      "Found test data d\n",
      "Found test data e\n"
     ]
    }
   ],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df, test_df_a, test_df_b, test_df_c, test_df_d, test_df_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-LSyABLG4_fA"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, test_loader_a, test_loader_b, test_loader_c, test_loader_d, test_loader_e = mnli_dataset.get_data_loaders(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e8a66bb5d7614602ac93db9cac811135",
      "0504a8ce7be543a49611a424fcfc9ca7",
      "bf065a9136dc4262a74c04f746b7c143",
      "6f61782328964e4dbdea05bee3797ee9",
      "814534237a9144deb7d52e4ceffabc43",
      "43f942f812884ace90aa9b1dd41a3d9a",
      "9d383d03f9304bce914326ea0e5d472b",
      "6b127ca5fb5a4d23b6c3ffe654cdfcaf",
      "fb792d8fbadc4bb6977a1986a9783e15",
      "373af73466be45f1bb216e6abbdb15be",
      "6497737147ce458b881bbb1b17492364"
     ]
    },
    "id": "qOdc4Cs2DEjt",
    "outputId": "7cdbd55e-a6e6-4cad-e765-de1a18e3f0c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jxzpENXlEh-u"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "is1TqwTREid9"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79aI1dun4_fB",
    "outputId": "eb5cbe4e-f9fa-4618-d840-611d259037f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,483,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qhU855Cw4_fB"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OfhYO7Db4_fB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):  \n",
    "  total_step = len(train_loader)\n",
    "  #model.load_state_dict(torch.load('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Weights/nli_0.pt'))\n",
    "  #model.load_state_dict(torch.load('./Weights/nli_3.pt'))\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      print(batch_idx, len(train_loader))\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        # loss = criterion(prediction, labels)\n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    #torch.save(model.state_dict(),os.path.join('/content/drive/MyDrive/COMPSCI 685 Advanced Natural Language Processing/Project/Weights', 'nli_{}.pt'.format(epoch)))\n",
    "    torch.save(model.state_dict(),os.path.join('./Weights', 'nli_new_{}.pt'.format(epoch)))\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TisZ5o86nkMW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def test(model, test_loader):\n",
    "  model.load_state_dict(torch.load('Weights/nli_new_4.pt', map_location=torch.device('cpu')))\n",
    "  model.eval()\n",
    "  total_test_acc  = 0\n",
    "  total_test_loss = 0\n",
    "  start = time.time()\n",
    "  preds = []\n",
    "  acts = []\n",
    "  with torch.no_grad():\n",
    "    #print(\"Prediction\", \"Label\")\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(test_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                           token_type_ids=seg_ids, \n",
    "                           attention_mask=mask_ids, \n",
    "                           labels=labels).values()\n",
    "        \n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "      preds = preds + prediction.tolist()\n",
    "      acts = acts + labels.tolist()\n",
    "      #print(prediction, labels)\n",
    "      total_test_loss += loss.item()\n",
    "      total_test_acc  += acc.item() \n",
    "\n",
    "  test_acc  = total_test_acc/len(test_loader)\n",
    "  test_loss = total_test_loss/len(test_loader)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "  print(f'test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  return [0 if preds[i][0] > preds[i][1] else 1 for i in range(len(preds))], acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPIxT4RB4_fC",
    "outputId": "9af2f2cb-4099-4673-f834-18bcc9bfcac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 366\n",
      "1 366\n",
      "2 366\n",
      "3 366\n",
      "4 366\n",
      "5 366\n",
      "6 366\n",
      "7 366\n",
      "8 366\n",
      "9 366\n",
      "10 366\n",
      "11 366\n",
      "12 366\n",
      "13 366\n",
      "14 366\n",
      "15 366\n",
      "16 366\n",
      "17 366\n",
      "18 366\n",
      "19 366\n",
      "20 366\n",
      "21 366\n",
      "22 366\n",
      "23 366\n",
      "24 366\n",
      "25 366\n",
      "26 366\n",
      "27 366\n",
      "28 366\n",
      "29 366\n",
      "30 366\n",
      "31 366\n",
      "32 366\n",
      "33 366\n",
      "34 366\n",
      "35 366\n",
      "36 366\n",
      "37 366\n",
      "38 366\n",
      "39 366\n",
      "40 366\n",
      "41 366\n",
      "42 366\n",
      "43 366\n",
      "44 366\n",
      "45 366\n",
      "46 366\n",
      "47 366\n",
      "48 366\n",
      "49 366\n",
      "50 366\n",
      "51 366\n",
      "52 366\n",
      "53 366\n",
      "54 366\n",
      "55 366\n",
      "56 366\n",
      "57 366\n",
      "58 366\n",
      "59 366\n",
      "60 366\n",
      "61 366\n",
      "62 366\n",
      "63 366\n",
      "64 366\n",
      "65 366\n",
      "66 366\n",
      "67 366\n",
      "68 366\n",
      "69 366\n",
      "70 366\n",
      "71 366\n",
      "72 366\n",
      "73 366\n",
      "74 366\n",
      "75 366\n",
      "76 366\n",
      "77 366\n",
      "78 366\n",
      "79 366\n",
      "80 366\n",
      "81 366\n",
      "82 366\n",
      "83 366\n",
      "84 366\n",
      "85 366\n",
      "86 366\n",
      "87 366\n",
      "88 366\n",
      "89 366\n",
      "90 366\n",
      "91 366\n",
      "92 366\n",
      "93 366\n",
      "94 366\n",
      "95 366\n",
      "96 366\n",
      "97 366\n",
      "98 366\n",
      "99 366\n",
      "100 366\n",
      "101 366\n",
      "102 366\n",
      "103 366\n",
      "104 366\n",
      "105 366\n",
      "106 366\n",
      "107 366\n",
      "108 366\n",
      "109 366\n",
      "110 366\n",
      "111 366\n",
      "112 366\n",
      "113 366\n",
      "114 366\n",
      "115 366\n",
      "116 366\n",
      "117 366\n",
      "118 366\n",
      "119 366\n",
      "120 366\n",
      "121 366\n",
      "122 366\n",
      "123 366\n",
      "124 366\n",
      "125 366\n",
      "126 366\n",
      "127 366\n",
      "128 366\n",
      "129 366\n",
      "130 366\n",
      "131 366\n",
      "132 366\n",
      "133 366\n",
      "134 366\n",
      "135 366\n",
      "136 366\n",
      "137 366\n",
      "138 366\n",
      "139 366\n",
      "140 366\n",
      "141 366\n",
      "142 366\n",
      "143 366\n",
      "144 366\n",
      "145 366\n",
      "146 366\n",
      "147 366\n",
      "148 366\n",
      "149 366\n",
      "150 366\n",
      "151 366\n",
      "152 366\n",
      "153 366\n",
      "154 366\n",
      "155 366\n",
      "156 366\n",
      "157 366\n",
      "158 366\n",
      "159 366\n",
      "160 366\n",
      "161 366\n",
      "162 366\n",
      "163 366\n",
      "164 366\n",
      "165 366\n",
      "166 366\n",
      "167 366\n",
      "168 366\n",
      "169 366\n",
      "170 366\n",
      "171 366\n",
      "172 366\n",
      "173 366\n",
      "174 366\n",
      "175 366\n",
      "176 366\n",
      "177 366\n",
      "178 366\n",
      "179 366\n",
      "180 366\n",
      "181 366\n",
      "182 366\n",
      "183 366\n",
      "184 366\n",
      "185 366\n",
      "186 366\n",
      "187 366\n",
      "188 366\n",
      "189 366\n",
      "190 366\n",
      "191 366\n",
      "192 366\n",
      "193 366\n",
      "194 366\n",
      "195 366\n",
      "196 366\n",
      "197 366\n",
      "198 366\n",
      "199 366\n",
      "200 366\n",
      "201 366\n",
      "202 366\n",
      "203 366\n",
      "204 366\n",
      "205 366\n",
      "206 366\n",
      "207 366\n",
      "208 366\n",
      "209 366\n",
      "210 366\n",
      "211 366\n",
      "212 366\n",
      "213 366\n",
      "214 366\n",
      "215 366\n",
      "216 366\n",
      "217 366\n",
      "218 366\n",
      "219 366\n",
      "220 366\n",
      "221 366\n",
      "222 366\n",
      "223 366\n",
      "224 366\n",
      "225 366\n",
      "226 366\n",
      "227 366\n",
      "228 366\n",
      "229 366\n",
      "230 366\n",
      "231 366\n",
      "232 366\n",
      "233 366\n",
      "234 366\n",
      "235 366\n",
      "236 366\n",
      "237 366\n",
      "238 366\n",
      "239 366\n",
      "240 366\n",
      "241 366\n",
      "242 366\n",
      "243 366\n",
      "244 366\n",
      "245 366\n",
      "246 366\n",
      "247 366\n",
      "248 366\n",
      "249 366\n",
      "250 366\n",
      "251 366\n",
      "252 366\n",
      "253 366\n",
      "254 366\n",
      "255 366\n",
      "256 366\n",
      "257 366\n",
      "258 366\n",
      "259 366\n",
      "260 366\n",
      "261 366\n",
      "262 366\n",
      "263 366\n",
      "264 366\n",
      "265 366\n",
      "266 366\n",
      "267 366\n",
      "268 366\n",
      "269 366\n",
      "270 366\n",
      "271 366\n",
      "272 366\n",
      "273 366\n",
      "274 366\n",
      "275 366\n",
      "276 366\n",
      "277 366\n",
      "278 366\n",
      "279 366\n",
      "280 366\n",
      "281 366\n",
      "282 366\n",
      "283 366\n",
      "284 366\n",
      "285 366\n",
      "286 366\n",
      "287 366\n",
      "288 366\n",
      "289 366\n",
      "290 366\n",
      "291 366\n",
      "292 366\n",
      "293 366\n",
      "294 366\n",
      "295 366\n",
      "296 366\n",
      "297 366\n",
      "298 366\n",
      "299 366\n",
      "300 366\n",
      "301 366\n",
      "302 366\n",
      "303 366\n",
      "304 366\n",
      "305 366\n",
      "306 366\n",
      "307 366\n",
      "308 366\n",
      "309 366\n",
      "310 366\n",
      "311 366\n",
      "312 366\n",
      "313 366\n",
      "314 366\n",
      "315 366\n",
      "316 366\n",
      "317 366\n",
      "318 366\n",
      "319 366\n",
      "320 366\n",
      "321 366\n",
      "322 366\n",
      "323 366\n",
      "324 366\n",
      "325 366\n",
      "326 366\n",
      "327 366\n",
      "328 366\n",
      "329 366\n",
      "330 366\n",
      "331 366\n",
      "332 366\n",
      "333 366\n",
      "334 366\n",
      "335 366\n",
      "336 366\n",
      "337 366\n",
      "338 366\n",
      "339 366\n",
      "340 366\n",
      "341 366\n",
      "342 366\n",
      "343 366\n",
      "344 366\n",
      "345 366\n",
      "346 366\n",
      "347 366\n",
      "348 366\n",
      "349 366\n",
      "350 366\n",
      "351 366\n",
      "352 366\n",
      "353 366\n",
      "354 366\n",
      "355 366\n",
      "356 366\n",
      "357 366\n",
      "358 366\n",
      "359 366\n",
      "360 366\n",
      "361 366\n",
      "362 366\n",
      "363 366\n",
      "364 366\n",
      "365 366\n",
      "Epoch 1: train_loss: 0.4727 train_acc: 0.7831 | val_loss: 0.4086 val_acc: 0.8214\n",
      "00:10:33.95\n",
      "0 366\n",
      "1 366\n",
      "2 366\n",
      "3 366\n",
      "4 366\n",
      "5 366\n",
      "6 366\n",
      "7 366\n",
      "8 366\n",
      "9 366\n",
      "10 366\n",
      "11 366\n",
      "12 366\n",
      "13 366\n",
      "14 366\n",
      "15 366\n",
      "16 366\n",
      "17 366\n",
      "18 366\n",
      "19 366\n",
      "20 366\n",
      "21 366\n",
      "22 366\n",
      "23 366\n",
      "24 366\n",
      "25 366\n",
      "26 366\n",
      "27 366\n",
      "28 366\n",
      "29 366\n",
      "30 366\n",
      "31 366\n",
      "32 366\n",
      "33 366\n",
      "34 366\n",
      "35 366\n",
      "36 366\n",
      "37 366\n",
      "38 366\n",
      "39 366\n",
      "40 366\n",
      "41 366\n",
      "42 366\n",
      "43 366\n",
      "44 366\n",
      "45 366\n",
      "46 366\n",
      "47 366\n",
      "48 366\n",
      "49 366\n",
      "50 366\n",
      "51 366\n",
      "52 366\n",
      "53 366\n",
      "54 366\n",
      "55 366\n",
      "56 366\n",
      "57 366\n",
      "58 366\n",
      "59 366\n",
      "60 366\n",
      "61 366\n",
      "62 366\n",
      "63 366\n",
      "64 366\n",
      "65 366\n",
      "66 366\n",
      "67 366\n",
      "68 366\n",
      "69 366\n",
      "70 366\n",
      "71 366\n",
      "72 366\n",
      "73 366\n",
      "74 366\n",
      "75 366\n",
      "76 366\n",
      "77 366\n",
      "78 366\n",
      "79 366\n",
      "80 366\n",
      "81 366\n",
      "82 366\n",
      "83 366\n",
      "84 366\n",
      "85 366\n",
      "86 366\n",
      "87 366\n",
      "88 366\n",
      "89 366\n",
      "90 366\n",
      "91 366\n",
      "92 366\n",
      "93 366\n",
      "94 366\n",
      "95 366\n",
      "96 366\n",
      "97 366\n",
      "98 366\n",
      "99 366\n",
      "100 366\n",
      "101 366\n",
      "102 366\n",
      "103 366\n",
      "104 366\n",
      "105 366\n",
      "106 366\n",
      "107 366\n",
      "108 366\n",
      "109 366\n",
      "110 366\n",
      "111 366\n",
      "112 366\n",
      "113 366\n",
      "114 366\n",
      "115 366\n",
      "116 366\n",
      "117 366\n",
      "118 366\n",
      "119 366\n",
      "120 366\n",
      "121 366\n",
      "122 366\n",
      "123 366\n",
      "124 366\n",
      "125 366\n",
      "126 366\n",
      "127 366\n",
      "128 366\n",
      "129 366\n",
      "130 366\n",
      "131 366\n",
      "132 366\n",
      "133 366\n",
      "134 366\n",
      "135 366\n",
      "136 366\n",
      "137 366\n",
      "138 366\n",
      "139 366\n",
      "140 366\n",
      "141 366\n",
      "142 366\n",
      "143 366\n",
      "144 366\n",
      "145 366\n",
      "146 366\n",
      "147 366\n",
      "148 366\n",
      "149 366\n",
      "150 366\n",
      "151 366\n",
      "152 366\n",
      "153 366\n",
      "154 366\n",
      "155 366\n",
      "156 366\n",
      "157 366\n",
      "158 366\n",
      "159 366\n",
      "160 366\n",
      "161 366\n",
      "162 366\n",
      "163 366\n",
      "164 366\n",
      "165 366\n",
      "166 366\n",
      "167 366\n",
      "168 366\n",
      "169 366\n",
      "170 366\n",
      "171 366\n",
      "172 366\n",
      "173 366\n",
      "174 366\n",
      "175 366\n",
      "176 366\n",
      "177 366\n",
      "178 366\n",
      "179 366\n",
      "180 366\n",
      "181 366\n",
      "182 366\n",
      "183 366\n",
      "184 366\n",
      "185 366\n",
      "186 366\n",
      "187 366\n",
      "188 366\n",
      "189 366\n",
      "190 366\n",
      "191 366\n",
      "192 366\n",
      "193 366\n",
      "194 366\n",
      "195 366\n",
      "196 366\n",
      "197 366\n",
      "198 366\n",
      "199 366\n",
      "200 366\n",
      "201 366\n",
      "202 366\n",
      "203 366\n",
      "204 366\n",
      "205 366\n",
      "206 366\n",
      "207 366\n",
      "208 366\n",
      "209 366\n",
      "210 366\n",
      "211 366\n",
      "212 366\n",
      "213 366\n",
      "214 366\n",
      "215 366\n",
      "216 366\n",
      "217 366\n",
      "218 366\n",
      "219 366\n",
      "220 366\n",
      "221 366\n",
      "222 366\n",
      "223 366\n",
      "224 366\n",
      "225 366\n",
      "226 366\n",
      "227 366\n",
      "228 366\n",
      "229 366\n",
      "230 366\n",
      "231 366\n",
      "232 366\n",
      "233 366\n",
      "234 366\n",
      "235 366\n",
      "236 366\n",
      "237 366\n",
      "238 366\n",
      "239 366\n",
      "240 366\n",
      "241 366\n",
      "242 366\n",
      "243 366\n",
      "244 366\n",
      "245 366\n",
      "246 366\n",
      "247 366\n",
      "248 366\n",
      "249 366\n",
      "250 366\n",
      "251 366\n",
      "252 366\n",
      "253 366\n",
      "254 366\n",
      "255 366\n",
      "256 366\n",
      "257 366\n",
      "258 366\n",
      "259 366\n",
      "260 366\n",
      "261 366\n",
      "262 366\n",
      "263 366\n",
      "264 366\n",
      "265 366\n",
      "266 366\n",
      "267 366\n",
      "268 366\n",
      "269 366\n",
      "270 366\n",
      "271 366\n",
      "272 366\n",
      "273 366\n",
      "274 366\n",
      "275 366\n",
      "276 366\n",
      "277 366\n",
      "278 366\n",
      "279 366\n",
      "280 366\n",
      "281 366\n",
      "282 366\n",
      "283 366\n",
      "284 366\n",
      "285 366\n",
      "286 366\n",
      "287 366\n",
      "288 366\n",
      "289 366\n",
      "290 366\n",
      "291 366\n",
      "292 366\n",
      "293 366\n",
      "294 366\n",
      "295 366\n",
      "296 366\n",
      "297 366\n",
      "298 366\n",
      "299 366\n",
      "300 366\n",
      "301 366\n",
      "302 366\n",
      "303 366\n",
      "304 366\n",
      "305 366\n",
      "306 366\n",
      "307 366\n",
      "308 366\n",
      "309 366\n",
      "310 366\n",
      "311 366\n",
      "312 366\n",
      "313 366\n",
      "314 366\n",
      "315 366\n",
      "316 366\n",
      "317 366\n",
      "318 366\n",
      "319 366\n",
      "320 366\n",
      "321 366\n",
      "322 366\n",
      "323 366\n",
      "324 366\n",
      "325 366\n",
      "326 366\n",
      "327 366\n",
      "328 366\n",
      "329 366\n",
      "330 366\n",
      "331 366\n",
      "332 366\n",
      "333 366\n",
      "334 366\n",
      "335 366\n",
      "336 366\n",
      "337 366\n",
      "338 366\n",
      "339 366\n",
      "340 366\n",
      "341 366\n",
      "342 366\n",
      "343 366\n",
      "344 366\n",
      "345 366\n",
      "346 366\n",
      "347 366\n",
      "348 366\n",
      "349 366\n",
      "350 366\n",
      "351 366\n",
      "352 366\n",
      "353 366\n",
      "354 366\n",
      "355 366\n",
      "356 366\n",
      "357 366\n",
      "358 366\n",
      "359 366\n",
      "360 366\n",
      "361 366\n",
      "362 366\n",
      "363 366\n",
      "364 366\n",
      "365 366\n",
      "Epoch 2: train_loss: 0.3564 train_acc: 0.8523 | val_loss: 0.3917 val_acc: 0.8359\n",
      "00:10:34.57\n",
      "0 366\n",
      "1 366\n",
      "2 366\n",
      "3 366\n",
      "4 366\n",
      "5 366\n",
      "6 366\n",
      "7 366\n",
      "8 366\n",
      "9 366\n",
      "10 366\n",
      "11 366\n",
      "12 366\n",
      "13 366\n",
      "14 366\n",
      "15 366\n",
      "16 366\n",
      "17 366\n",
      "18 366\n",
      "19 366\n",
      "20 366\n",
      "21 366\n",
      "22 366\n",
      "23 366\n",
      "24 366\n",
      "25 366\n",
      "26 366\n",
      "27 366\n",
      "28 366\n",
      "29 366\n",
      "30 366\n",
      "31 366\n",
      "32 366\n",
      "33 366\n",
      "34 366\n",
      "35 366\n",
      "36 366\n",
      "37 366\n",
      "38 366\n",
      "39 366\n",
      "40 366\n",
      "41 366\n",
      "42 366\n",
      "43 366\n",
      "44 366\n",
      "45 366\n",
      "46 366\n",
      "47 366\n",
      "48 366\n",
      "49 366\n",
      "50 366\n",
      "51 366\n",
      "52 366\n",
      "53 366\n",
      "54 366\n",
      "55 366\n",
      "56 366\n",
      "57 366\n",
      "58 366\n",
      "59 366\n",
      "60 366\n",
      "61 366\n",
      "62 366\n",
      "63 366\n",
      "64 366\n",
      "65 366\n",
      "66 366\n",
      "67 366\n",
      "68 366\n",
      "69 366\n",
      "70 366\n",
      "71 366\n",
      "72 366\n",
      "73 366\n",
      "74 366\n",
      "75 366\n",
      "76 366\n",
      "77 366\n",
      "78 366\n",
      "79 366\n",
      "80 366\n",
      "81 366\n",
      "82 366\n",
      "83 366\n",
      "84 366\n",
      "85 366\n",
      "86 366\n",
      "87 366\n",
      "88 366\n",
      "89 366\n",
      "90 366\n",
      "91 366\n",
      "92 366\n",
      "93 366\n",
      "94 366\n",
      "95 366\n",
      "96 366\n",
      "97 366\n",
      "98 366\n",
      "99 366\n",
      "100 366\n",
      "101 366\n",
      "102 366\n",
      "103 366\n",
      "104 366\n",
      "105 366\n",
      "106 366\n",
      "107 366\n",
      "108 366\n",
      "109 366\n",
      "110 366\n",
      "111 366\n",
      "112 366\n",
      "113 366\n",
      "114 366\n",
      "115 366\n",
      "116 366\n",
      "117 366\n",
      "118 366\n",
      "119 366\n",
      "120 366\n",
      "121 366\n",
      "122 366\n",
      "123 366\n",
      "124 366\n",
      "125 366\n",
      "126 366\n",
      "127 366\n",
      "128 366\n",
      "129 366\n",
      "130 366\n",
      "131 366\n",
      "132 366\n",
      "133 366\n",
      "134 366\n",
      "135 366\n",
      "136 366\n",
      "137 366\n",
      "138 366\n",
      "139 366\n",
      "140 366\n",
      "141 366\n",
      "142 366\n",
      "143 366\n",
      "144 366\n",
      "145 366\n",
      "146 366\n",
      "147 366\n",
      "148 366\n",
      "149 366\n",
      "150 366\n",
      "151 366\n",
      "152 366\n",
      "153 366\n",
      "154 366\n",
      "155 366\n",
      "156 366\n",
      "157 366\n",
      "158 366\n",
      "159 366\n",
      "160 366\n",
      "161 366\n",
      "162 366\n",
      "163 366\n",
      "164 366\n",
      "165 366\n",
      "166 366\n",
      "167 366\n",
      "168 366\n",
      "169 366\n",
      "170 366\n",
      "171 366\n",
      "172 366\n",
      "173 366\n",
      "174 366\n",
      "175 366\n",
      "176 366\n",
      "177 366\n",
      "178 366\n",
      "179 366\n",
      "180 366\n",
      "181 366\n",
      "182 366\n",
      "183 366\n",
      "184 366\n",
      "185 366\n",
      "186 366\n",
      "187 366\n",
      "188 366\n",
      "189 366\n",
      "190 366\n",
      "191 366\n",
      "192 366\n",
      "193 366\n",
      "194 366\n",
      "195 366\n",
      "196 366\n",
      "197 366\n",
      "198 366\n",
      "199 366\n",
      "200 366\n",
      "201 366\n",
      "202 366\n",
      "203 366\n",
      "204 366\n",
      "205 366\n",
      "206 366\n",
      "207 366\n",
      "208 366\n",
      "209 366\n",
      "210 366\n",
      "211 366\n",
      "212 366\n",
      "213 366\n",
      "214 366\n",
      "215 366\n",
      "216 366\n",
      "217 366\n",
      "218 366\n",
      "219 366\n",
      "220 366\n",
      "221 366\n",
      "222 366\n",
      "223 366\n",
      "224 366\n",
      "225 366\n",
      "226 366\n",
      "227 366\n",
      "228 366\n",
      "229 366\n",
      "230 366\n",
      "231 366\n",
      "232 366\n",
      "233 366\n",
      "234 366\n",
      "235 366\n",
      "236 366\n",
      "237 366\n",
      "238 366\n",
      "239 366\n",
      "240 366\n",
      "241 366\n",
      "242 366\n",
      "243 366\n",
      "244 366\n",
      "245 366\n",
      "246 366\n",
      "247 366\n",
      "248 366\n",
      "249 366\n",
      "250 366\n",
      "251 366\n",
      "252 366\n",
      "253 366\n",
      "254 366\n",
      "255 366\n",
      "256 366\n",
      "257 366\n",
      "258 366\n",
      "259 366\n",
      "260 366\n",
      "261 366\n",
      "262 366\n",
      "263 366\n",
      "264 366\n",
      "265 366\n",
      "266 366\n",
      "267 366\n",
      "268 366\n",
      "269 366\n",
      "270 366\n",
      "271 366\n",
      "272 366\n",
      "273 366\n",
      "274 366\n",
      "275 366\n",
      "276 366\n",
      "277 366\n",
      "278 366\n",
      "279 366\n",
      "280 366\n",
      "281 366\n",
      "282 366\n",
      "283 366\n",
      "284 366\n",
      "285 366\n",
      "286 366\n",
      "287 366\n",
      "288 366\n",
      "289 366\n",
      "290 366\n",
      "291 366\n",
      "292 366\n",
      "293 366\n",
      "294 366\n",
      "295 366\n",
      "296 366\n",
      "297 366\n",
      "298 366\n",
      "299 366\n",
      "300 366\n",
      "301 366\n",
      "302 366\n",
      "303 366\n",
      "304 366\n",
      "305 366\n",
      "306 366\n",
      "307 366\n",
      "308 366\n",
      "309 366\n",
      "310 366\n",
      "311 366\n",
      "312 366\n",
      "313 366\n",
      "314 366\n",
      "315 366\n",
      "316 366\n",
      "317 366\n",
      "318 366\n",
      "319 366\n",
      "320 366\n",
      "321 366\n",
      "322 366\n",
      "323 366\n",
      "324 366\n",
      "325 366\n",
      "326 366\n",
      "327 366\n",
      "328 366\n",
      "329 366\n",
      "330 366\n",
      "331 366\n",
      "332 366\n",
      "333 366\n",
      "334 366\n",
      "335 366\n",
      "336 366\n",
      "337 366\n",
      "338 366\n",
      "339 366\n",
      "340 366\n",
      "341 366\n",
      "342 366\n",
      "343 366\n",
      "344 366\n",
      "345 366\n",
      "346 366\n",
      "347 366\n",
      "348 366\n",
      "349 366\n",
      "350 366\n",
      "351 366\n",
      "352 366\n",
      "353 366\n",
      "354 366\n",
      "355 366\n",
      "356 366\n",
      "357 366\n",
      "358 366\n",
      "359 366\n",
      "360 366\n",
      "361 366\n",
      "362 366\n",
      "363 366\n",
      "364 366\n",
      "365 366\n",
      "Epoch 3: train_loss: 0.2659 train_acc: 0.8971 | val_loss: 0.4303 val_acc: 0.8319\n",
      "00:10:34.76\n",
      "0 366\n",
      "1 366\n",
      "2 366\n",
      "3 366\n",
      "4 366\n",
      "5 366\n",
      "6 366\n",
      "7 366\n",
      "8 366\n",
      "9 366\n",
      "10 366\n",
      "11 366\n",
      "12 366\n",
      "13 366\n",
      "14 366\n",
      "15 366\n",
      "16 366\n",
      "17 366\n",
      "18 366\n",
      "19 366\n",
      "20 366\n",
      "21 366\n",
      "22 366\n",
      "23 366\n",
      "24 366\n",
      "25 366\n",
      "26 366\n",
      "27 366\n",
      "28 366\n",
      "29 366\n",
      "30 366\n",
      "31 366\n",
      "32 366\n",
      "33 366\n",
      "34 366\n",
      "35 366\n",
      "36 366\n",
      "37 366\n",
      "38 366\n",
      "39 366\n",
      "40 366\n",
      "41 366\n",
      "42 366\n",
      "43 366\n",
      "44 366\n",
      "45 366\n",
      "46 366\n",
      "47 366\n",
      "48 366\n",
      "49 366\n",
      "50 366\n",
      "51 366\n",
      "52 366\n",
      "53 366\n",
      "54 366\n",
      "55 366\n",
      "56 366\n",
      "57 366\n",
      "58 366\n",
      "59 366\n",
      "60 366\n",
      "61 366\n",
      "62 366\n",
      "63 366\n",
      "64 366\n",
      "65 366\n",
      "66 366\n",
      "67 366\n",
      "68 366\n",
      "69 366\n",
      "70 366\n",
      "71 366\n",
      "72 366\n",
      "73 366\n",
      "74 366\n",
      "75 366\n",
      "76 366\n",
      "77 366\n",
      "78 366\n",
      "79 366\n",
      "80 366\n",
      "81 366\n",
      "82 366\n",
      "83 366\n",
      "84 366\n",
      "85 366\n",
      "86 366\n",
      "87 366\n",
      "88 366\n",
      "89 366\n",
      "90 366\n",
      "91 366\n",
      "92 366\n",
      "93 366\n",
      "94 366\n",
      "95 366\n",
      "96 366\n",
      "97 366\n",
      "98 366\n",
      "99 366\n",
      "100 366\n",
      "101 366\n",
      "102 366\n",
      "103 366\n",
      "104 366\n",
      "105 366\n",
      "106 366\n",
      "107 366\n",
      "108 366\n",
      "109 366\n",
      "110 366\n",
      "111 366\n",
      "112 366\n",
      "113 366\n",
      "114 366\n",
      "115 366\n",
      "116 366\n",
      "117 366\n",
      "118 366\n",
      "119 366\n",
      "120 366\n",
      "121 366\n",
      "122 366\n",
      "123 366\n",
      "124 366\n",
      "125 366\n",
      "126 366\n",
      "127 366\n",
      "128 366\n",
      "129 366\n",
      "130 366\n",
      "131 366\n",
      "132 366\n",
      "133 366\n",
      "134 366\n",
      "135 366\n",
      "136 366\n",
      "137 366\n",
      "138 366\n",
      "139 366\n",
      "140 366\n",
      "141 366\n",
      "142 366\n",
      "143 366\n",
      "144 366\n",
      "145 366\n",
      "146 366\n",
      "147 366\n",
      "148 366\n",
      "149 366\n",
      "150 366\n",
      "151 366\n",
      "152 366\n",
      "153 366\n",
      "154 366\n",
      "155 366\n",
      "156 366\n",
      "157 366\n",
      "158 366\n",
      "159 366\n",
      "160 366\n",
      "161 366\n",
      "162 366\n",
      "163 366\n",
      "164 366\n",
      "165 366\n",
      "166 366\n",
      "167 366\n",
      "168 366\n",
      "169 366\n",
      "170 366\n",
      "171 366\n",
      "172 366\n",
      "173 366\n",
      "174 366\n",
      "175 366\n",
      "176 366\n",
      "177 366\n",
      "178 366\n",
      "179 366\n",
      "180 366\n",
      "181 366\n",
      "182 366\n",
      "183 366\n",
      "184 366\n",
      "185 366\n",
      "186 366\n",
      "187 366\n",
      "188 366\n",
      "189 366\n",
      "190 366\n",
      "191 366\n",
      "192 366\n",
      "193 366\n",
      "194 366\n",
      "195 366\n",
      "196 366\n",
      "197 366\n",
      "198 366\n",
      "199 366\n",
      "200 366\n",
      "201 366\n",
      "202 366\n",
      "203 366\n",
      "204 366\n",
      "205 366\n",
      "206 366\n",
      "207 366\n",
      "208 366\n",
      "209 366\n",
      "210 366\n",
      "211 366\n",
      "212 366\n",
      "213 366\n",
      "214 366\n",
      "215 366\n",
      "216 366\n",
      "217 366\n",
      "218 366\n",
      "219 366\n",
      "220 366\n",
      "221 366\n",
      "222 366\n",
      "223 366\n",
      "224 366\n",
      "225 366\n",
      "226 366\n",
      "227 366\n",
      "228 366\n",
      "229 366\n",
      "230 366\n",
      "231 366\n",
      "232 366\n",
      "233 366\n",
      "234 366\n",
      "235 366\n",
      "236 366\n",
      "237 366\n",
      "238 366\n",
      "239 366\n",
      "240 366\n",
      "241 366\n",
      "242 366\n",
      "243 366\n",
      "244 366\n",
      "245 366\n",
      "246 366\n",
      "247 366\n",
      "248 366\n",
      "249 366\n",
      "250 366\n",
      "251 366\n",
      "252 366\n",
      "253 366\n",
      "254 366\n",
      "255 366\n",
      "256 366\n",
      "257 366\n",
      "258 366\n",
      "259 366\n",
      "260 366\n",
      "261 366\n",
      "262 366\n",
      "263 366\n",
      "264 366\n",
      "265 366\n",
      "266 366\n",
      "267 366\n",
      "268 366\n",
      "269 366\n",
      "270 366\n",
      "271 366\n",
      "272 366\n",
      "273 366\n",
      "274 366\n",
      "275 366\n",
      "276 366\n",
      "277 366\n",
      "278 366\n",
      "279 366\n",
      "280 366\n",
      "281 366\n",
      "282 366\n",
      "283 366\n",
      "284 366\n",
      "285 366\n",
      "286 366\n",
      "287 366\n",
      "288 366\n",
      "289 366\n",
      "290 366\n",
      "291 366\n",
      "292 366\n",
      "293 366\n",
      "294 366\n",
      "295 366\n",
      "296 366\n",
      "297 366\n",
      "298 366\n",
      "299 366\n",
      "300 366\n",
      "301 366\n",
      "302 366\n",
      "303 366\n",
      "304 366\n",
      "305 366\n",
      "306 366\n",
      "307 366\n",
      "308 366\n",
      "309 366\n",
      "310 366\n",
      "311 366\n",
      "312 366\n",
      "313 366\n",
      "314 366\n",
      "315 366\n",
      "316 366\n",
      "317 366\n",
      "318 366\n",
      "319 366\n",
      "320 366\n",
      "321 366\n",
      "322 366\n",
      "323 366\n",
      "324 366\n",
      "325 366\n",
      "326 366\n",
      "327 366\n",
      "328 366\n",
      "329 366\n",
      "330 366\n",
      "331 366\n",
      "332 366\n",
      "333 366\n",
      "334 366\n",
      "335 366\n",
      "336 366\n",
      "337 366\n",
      "338 366\n",
      "339 366\n",
      "340 366\n",
      "341 366\n",
      "342 366\n",
      "343 366\n",
      "344 366\n",
      "345 366\n",
      "346 366\n",
      "347 366\n",
      "348 366\n",
      "349 366\n",
      "350 366\n",
      "351 366\n",
      "352 366\n",
      "353 366\n",
      "354 366\n",
      "355 366\n",
      "356 366\n",
      "357 366\n",
      "358 366\n",
      "359 366\n",
      "360 366\n",
      "361 366\n",
      "362 366\n",
      "363 366\n",
      "364 366\n",
      "365 366\n",
      "Epoch 4: train_loss: 0.1887 train_acc: 0.9297 | val_loss: 0.4725 val_acc: 0.8328\n",
      "00:10:34.53\n",
      "0 366\n",
      "1 366\n",
      "2 366\n",
      "3 366\n",
      "4 366\n",
      "5 366\n",
      "6 366\n",
      "7 366\n",
      "8 366\n",
      "9 366\n",
      "10 366\n",
      "11 366\n",
      "12 366\n",
      "13 366\n",
      "14 366\n",
      "15 366\n",
      "16 366\n",
      "17 366\n",
      "18 366\n",
      "19 366\n",
      "20 366\n",
      "21 366\n",
      "22 366\n",
      "23 366\n",
      "24 366\n",
      "25 366\n",
      "26 366\n",
      "27 366\n",
      "28 366\n",
      "29 366\n",
      "30 366\n",
      "31 366\n",
      "32 366\n",
      "33 366\n",
      "34 366\n",
      "35 366\n",
      "36 366\n",
      "37 366\n",
      "38 366\n",
      "39 366\n",
      "40 366\n",
      "41 366\n",
      "42 366\n",
      "43 366\n",
      "44 366\n",
      "45 366\n",
      "46 366\n",
      "47 366\n",
      "48 366\n",
      "49 366\n",
      "50 366\n",
      "51 366\n",
      "52 366\n",
      "53 366\n",
      "54 366\n",
      "55 366\n",
      "56 366\n",
      "57 366\n",
      "58 366\n",
      "59 366\n",
      "60 366\n",
      "61 366\n",
      "62 366\n",
      "63 366\n",
      "64 366\n",
      "65 366\n",
      "66 366\n",
      "67 366\n",
      "68 366\n",
      "69 366\n",
      "70 366\n",
      "71 366\n",
      "72 366\n",
      "73 366\n",
      "74 366\n",
      "75 366\n",
      "76 366\n",
      "77 366\n",
      "78 366\n",
      "79 366\n",
      "80 366\n",
      "81 366\n",
      "82 366\n",
      "83 366\n",
      "84 366\n",
      "85 366\n",
      "86 366\n",
      "87 366\n",
      "88 366\n",
      "89 366\n",
      "90 366\n",
      "91 366\n",
      "92 366\n",
      "93 366\n",
      "94 366\n",
      "95 366\n",
      "96 366\n",
      "97 366\n",
      "98 366\n",
      "99 366\n",
      "100 366\n",
      "101 366\n",
      "102 366\n",
      "103 366\n",
      "104 366\n",
      "105 366\n",
      "106 366\n",
      "107 366\n",
      "108 366\n",
      "109 366\n",
      "110 366\n",
      "111 366\n",
      "112 366\n",
      "113 366\n",
      "114 366\n",
      "115 366\n",
      "116 366\n",
      "117 366\n",
      "118 366\n",
      "119 366\n",
      "120 366\n",
      "121 366\n",
      "122 366\n",
      "123 366\n",
      "124 366\n",
      "125 366\n",
      "126 366\n",
      "127 366\n",
      "128 366\n",
      "129 366\n",
      "130 366\n",
      "131 366\n",
      "132 366\n",
      "133 366\n",
      "134 366\n",
      "135 366\n",
      "136 366\n",
      "137 366\n",
      "138 366\n",
      "139 366\n",
      "140 366\n",
      "141 366\n",
      "142 366\n",
      "143 366\n",
      "144 366\n",
      "145 366\n",
      "146 366\n",
      "147 366\n",
      "148 366\n",
      "149 366\n",
      "150 366\n",
      "151 366\n",
      "152 366\n",
      "153 366\n",
      "154 366\n",
      "155 366\n",
      "156 366\n",
      "157 366\n",
      "158 366\n",
      "159 366\n",
      "160 366\n",
      "161 366\n",
      "162 366\n",
      "163 366\n",
      "164 366\n",
      "165 366\n",
      "166 366\n",
      "167 366\n",
      "168 366\n",
      "169 366\n",
      "170 366\n",
      "171 366\n",
      "172 366\n",
      "173 366\n",
      "174 366\n",
      "175 366\n",
      "176 366\n",
      "177 366\n",
      "178 366\n",
      "179 366\n",
      "180 366\n",
      "181 366\n",
      "182 366\n",
      "183 366\n",
      "184 366\n",
      "185 366\n",
      "186 366\n",
      "187 366\n",
      "188 366\n",
      "189 366\n",
      "190 366\n",
      "191 366\n",
      "192 366\n",
      "193 366\n",
      "194 366\n",
      "195 366\n",
      "196 366\n",
      "197 366\n",
      "198 366\n",
      "199 366\n",
      "200 366\n",
      "201 366\n",
      "202 366\n",
      "203 366\n",
      "204 366\n",
      "205 366\n",
      "206 366\n",
      "207 366\n",
      "208 366\n",
      "209 366\n",
      "210 366\n",
      "211 366\n",
      "212 366\n",
      "213 366\n",
      "214 366\n",
      "215 366\n",
      "216 366\n",
      "217 366\n",
      "218 366\n",
      "219 366\n",
      "220 366\n",
      "221 366\n",
      "222 366\n",
      "223 366\n",
      "224 366\n",
      "225 366\n",
      "226 366\n",
      "227 366\n",
      "228 366\n",
      "229 366\n",
      "230 366\n",
      "231 366\n",
      "232 366\n",
      "233 366\n",
      "234 366\n",
      "235 366\n",
      "236 366\n",
      "237 366\n",
      "238 366\n",
      "239 366\n",
      "240 366\n",
      "241 366\n",
      "242 366\n",
      "243 366\n",
      "244 366\n",
      "245 366\n",
      "246 366\n",
      "247 366\n",
      "248 366\n",
      "249 366\n",
      "250 366\n",
      "251 366\n",
      "252 366\n",
      "253 366\n",
      "254 366\n",
      "255 366\n",
      "256 366\n",
      "257 366\n",
      "258 366\n",
      "259 366\n",
      "260 366\n",
      "261 366\n",
      "262 366\n",
      "263 366\n",
      "264 366\n",
      "265 366\n",
      "266 366\n",
      "267 366\n",
      "268 366\n",
      "269 366\n",
      "270 366\n",
      "271 366\n",
      "272 366\n",
      "273 366\n",
      "274 366\n",
      "275 366\n",
      "276 366\n",
      "277 366\n",
      "278 366\n",
      "279 366\n",
      "280 366\n",
      "281 366\n",
      "282 366\n",
      "283 366\n",
      "284 366\n",
      "285 366\n",
      "286 366\n",
      "287 366\n",
      "288 366\n",
      "289 366\n",
      "290 366\n",
      "291 366\n",
      "292 366\n",
      "293 366\n",
      "294 366\n",
      "295 366\n",
      "296 366\n",
      "297 366\n",
      "298 366\n",
      "299 366\n",
      "300 366\n",
      "301 366\n",
      "302 366\n",
      "303 366\n",
      "304 366\n",
      "305 366\n",
      "306 366\n",
      "307 366\n",
      "308 366\n",
      "309 366\n",
      "310 366\n",
      "311 366\n",
      "312 366\n",
      "313 366\n",
      "314 366\n",
      "315 366\n",
      "316 366\n",
      "317 366\n",
      "318 366\n",
      "319 366\n",
      "320 366\n",
      "321 366\n",
      "322 366\n",
      "323 366\n",
      "324 366\n",
      "325 366\n",
      "326 366\n",
      "327 366\n",
      "328 366\n",
      "329 366\n",
      "330 366\n",
      "331 366\n",
      "332 366\n",
      "333 366\n",
      "334 366\n",
      "335 366\n",
      "336 366\n",
      "337 366\n",
      "338 366\n",
      "339 366\n",
      "340 366\n",
      "341 366\n",
      "342 366\n",
      "343 366\n",
      "344 366\n",
      "345 366\n",
      "346 366\n",
      "347 366\n",
      "348 366\n",
      "349 366\n",
      "350 366\n",
      "351 366\n",
      "352 366\n",
      "353 366\n",
      "354 366\n",
      "355 366\n",
      "356 366\n",
      "357 366\n",
      "358 366\n",
      "359 366\n",
      "360 366\n",
      "361 366\n",
      "362 366\n",
      "363 366\n",
      "364 366\n",
      "365 366\n",
      "Epoch 5: train_loss: 0.1293 train_acc: 0.9529 | val_loss: 0.5725 val_acc: 0.8487\n",
      "00:10:34.44\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3nGBLgp312AD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.6206 test_acc: 0.8349\n",
      "00:00:26.37\n"
     ]
    }
   ],
   "source": [
    "preds, acts = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test accuracy is 0.8343013324222753\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(len(preds)):\n",
    "    acc += 1 if preds[i] == acts[i] else 0\n",
    "print(\"Overall test accuracy is\", acc/len(acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5w11ZMTvp6yr",
    "outputId": "879fa89b-bfca-4b6e-a749-d77eb8239ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.8757 test_acc: 0.7802\n",
      "00:00:09.94\n"
     ]
    }
   ],
   "source": [
    "preds_a, acts_a = test(model, test_loader_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total offensive examples: 414\n",
      "Correctly predicted offensive examples: 281\n",
      "Accuracy for predicting offensive examples: 0.678743961352657\n",
      "Total non offensive examples: 897\n",
      "Correctly predicted non offensive examples: 742\n",
      "Accuracy for predicting non offensive examples: 0.8272017837235228\n"
     ]
    }
   ],
   "source": [
    "ent_acts_a = 0\n",
    "cont_acts_a = 0\n",
    "correct_ents_a = 0\n",
    "correct_conts_a = 0\n",
    "for i in range(len(preds_a)):\n",
    "    if acts_a[i] == 0:\n",
    "        ent_acts_a += 1\n",
    "        if preds_a[i] == 0:\n",
    "            correct_ents_a += 1\n",
    "    else:\n",
    "        cont_acts_a += 1\n",
    "        if preds_a[i] == 1:\n",
    "            correct_conts_a += 1\n",
    "            \n",
    "print(\"Total offensive examples:\", ent_acts_a)\n",
    "print(\"Correctly predicted offensive examples:\", correct_ents_a)\n",
    "print(\"Accuracy for predicting offensive examples:\", correct_ents_a/ent_acts_a)\n",
    "print(\"Total non offensive examples:\", cont_acts_a)\n",
    "print(\"Correctly predicted non offensive examples:\", correct_conts_a)\n",
    "print(\"Accuracy for predicting non offensive examples:\", correct_conts_a/cont_acts_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "uqIU3lTPqLkV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.6044 test_acc: 0.8610\n",
      "00:00:02.78\n"
     ]
    }
   ],
   "source": [
    "preds_b, acts_b = test(model, test_loader_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total targeted offensive examples: 408\n",
      "Correctly predicted targeted offensive examples: 402\n",
      "Accuracy for predicting targeted offensive examples: 0.9852941176470589\n",
      "Total non targeted offensive examples: 61\n",
      "Correctly predicted non targeted offensive examples: 4\n",
      "Accuracy for predicting non targeted offensive examples: 0.06557377049180328\n"
     ]
    }
   ],
   "source": [
    "ent_acts_b = 0\n",
    "cont_acts_b = 0\n",
    "correct_ents_b = 0\n",
    "correct_conts_b = 0\n",
    "for i in range(len(preds_b)):\n",
    "    if acts_b[i] == 0:\n",
    "        ent_acts_b += 1\n",
    "        if preds_b[i] == 0:\n",
    "            correct_ents_b += 1\n",
    "    else:\n",
    "        cont_acts_b += 1\n",
    "        if preds_b[i] == 1:\n",
    "            correct_conts_b += 1\n",
    "            \n",
    "print(\"Total targeted offensive examples:\", ent_acts_b)\n",
    "print(\"Correctly predicted targeted offensive examples:\", correct_ents_b)\n",
    "print(\"Accuracy for predicting targeted offensive examples:\", correct_ents_b/ent_acts_b)\n",
    "print(\"Total non targeted offensive examples:\", cont_acts_b)\n",
    "print(\"Correctly predicted non targeted offensive examples:\", correct_conts_b)\n",
    "print(\"Accuracy for predicting non targeted offensive examples:\", correct_conts_b/cont_acts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[(train_df.sentence2 == \"This is targeted offense.\") & (train_df.gold_label == 'contradiction')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2967"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[(train_df.sentence2 == \"This is targeted offense.\") & (train_df.gold_label == 'entailment')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The above two cells is one possible explanation of why the model always predicts entailment for task_b'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The above two cells is one possible explanation of why the model always predicts entailment for task_b'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TyO6pUpqk1i",
    "outputId": "a1fa4716-4858-4ede-b8f5-89828ca0d40c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.5066 test_acc: 0.8659\n",
      "00:00:02.52\n"
     ]
    }
   ],
   "source": [
    "preds_c, acts_c = test(model, test_loader_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total individual targeted offensive examples: 224\n",
      "Correctly predicted individual targeted offensive examples: 198\n",
      "Accuracy for predicting individual targeted offensive examples: 0.8839285714285714\n",
      "Total non individual targeted offensive examples: 135\n",
      "Correctly predicted non individual targeted offensive examples: 114\n",
      "Accuracy for predicting non individual targeted offensive examples: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "ent_acts_c = 0\n",
    "cont_acts_c = 0\n",
    "correct_ents_c = 0\n",
    "correct_conts_c = 0\n",
    "for i in range(len(preds_c)):\n",
    "    if acts_c[i] == 0:\n",
    "        ent_acts_c += 1\n",
    "        if preds_c[i] == 0:\n",
    "            correct_ents_c += 1\n",
    "    else:\n",
    "        cont_acts_c += 1\n",
    "        if preds_c[i] == 1:\n",
    "            correct_conts_c += 1\n",
    "            \n",
    "print(\"Total individual targeted offensive examples:\", ent_acts_c)\n",
    "print(\"Correctly predicted individual targeted offensive examples:\", correct_ents_c)\n",
    "print(\"Accuracy for predicting individual targeted offensive examples:\", correct_ents_c/ent_acts_c)\n",
    "print(\"Total non individual targeted offensive examples:\", cont_acts_c)\n",
    "print(\"Correctly predicted non individual targeted offensive examples:\", correct_conts_c)\n",
    "print(\"Accuracy for predicting non individual targeted offensive examples:\", correct_conts_c/cont_acts_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZOu6kmwq9MM",
    "outputId": "8e8af0bb-8565-44fc-e2e6-1ecac75c810b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.3581 test_acc: 0.8594\n",
      "00:00:02.64\n"
     ]
    }
   ],
   "source": [
    "preds_d, acts_d = test(model, test_loader_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total group targeted offensive examples: 116\n",
      "Correctly predicted group targeted offensive examples: 94\n",
      "Accuracy for predicting group targeted offensive examples: 0.8103448275862069\n",
      "Total non group targeted offensive examples: 276\n",
      "Correctly predicted non group targeted offensive examples: 249\n",
      "Accuracy for predicting non group targeted offensive examples: 0.9021739130434783\n"
     ]
    }
   ],
   "source": [
    "ent_acts_d = 0\n",
    "cont_acts_d = 0\n",
    "correct_ents_d = 0\n",
    "correct_conts_d = 0\n",
    "for i in range(len(preds_d)):\n",
    "    if acts_d[i] == 0:\n",
    "        ent_acts_d += 1\n",
    "        if preds_d[i] == 0:\n",
    "            correct_ents_d += 1\n",
    "    else:\n",
    "        cont_acts_d += 1\n",
    "        if preds_d[i] == 1:\n",
    "            correct_conts_d += 1\n",
    "            \n",
    "print(\"Total group targeted offensive examples:\", ent_acts_d)\n",
    "print(\"Correctly predicted group targeted offensive examples:\", correct_ents_d)\n",
    "print(\"Accuracy for predicting group targeted offensive examples:\", correct_ents_d/ent_acts_d)\n",
    "print(\"Total non group targeted offensive examples:\", cont_acts_d)\n",
    "print(\"Correctly predicted non group targeted offensive examples:\", correct_conts_d)\n",
    "print(\"Accuracy for predicting non group targeted offensive examples:\", correct_conts_d/cont_acts_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUU1YuTIq_z3",
    "outputId": "b76f41c6-8267-4187-b31b-cd49df2c4f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2499 test_acc: 0.9055\n",
      "00:00:03.58\n"
     ]
    }
   ],
   "source": [
    "preds_e, acts_e = test(model, test_loader_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total other targeted offensive examples: 46\n",
      "Correctly predicted other targeted offensive examples: 9\n",
      "Accuracy for predicting other targeted offensive examples: 0.1956521739130435\n",
      "Total non other targeted offensive examples: 350\n",
      "Correctly predicted non other targeted offensive examples: 349\n",
      "Accuracy for predicting non other targeted offensive examples: 0.9971428571428571\n"
     ]
    }
   ],
   "source": [
    "ent_acts_e = 0\n",
    "cont_acts_e = 0\n",
    "correct_ents_e = 0\n",
    "correct_conts_e = 0\n",
    "for i in range(len(preds_e)):\n",
    "    if acts_e[i] == 0:\n",
    "        ent_acts_e += 1\n",
    "        if preds_e[i] == 0:\n",
    "            correct_ents_e += 1\n",
    "    else:\n",
    "        cont_acts_e += 1\n",
    "        if preds_e[i] == 1:\n",
    "            correct_conts_e += 1\n",
    "            \n",
    "print(\"Total other targeted offensive examples:\", ent_acts_e)\n",
    "print(\"Correctly predicted other targeted offensive examples:\", correct_ents_e)\n",
    "print(\"Accuracy for predicting other targeted offensive examples:\", correct_ents_e/ent_acts_e)\n",
    "print(\"Total non other targeted offensive examples:\", cont_acts_e)\n",
    "print(\"Correctly predicted non other targeted offensive examples:\", correct_conts_e)\n",
    "print(\"Accuracy for predicting non other targeted offensive examples:\", correct_conts_e/cont_acts_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[(train_df.sentence2 == \"This isn't targeted towards a group or an individual.\") & (train_df.gold_label == 'entailment')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[(train_df.sentence2 == \"This isn't targeted towards a group or an individual.\") & (train_df.gold_label == 'contradiction')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The above two examples give us an idea as to why the model might not be predicting any positive other targeted examples'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The above two examples give us an idea as to why the model might not be predicting any positive other targeted examples'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentence Entailment BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0183e10141f049a1820827c9dfbd6c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d01f56954ef74452b649def7b9adc937",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a4784a513b749d68959b4250a8a1593",
      "value": 28
     }
    },
    "0504a8ce7be543a49611a424fcfc9ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1543919f31dd46c6a967a412af4d21bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16b6fa8b34e24eedb7c6c80e9e57b5d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24f1399337dc4cf998e189d697b8e282": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e8aaf8e2a644b8384d07e86578e7624": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30c1af9f395f4f148a4038c1f13b2088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e7c6e812fdc4b36a25096e9f176f75a",
       "IPY_MODEL_bf7f2daca2264bd289670898d7ab10e4",
       "IPY_MODEL_3af999385a094ccc85eadf702ab69723"
      ],
      "layout": "IPY_MODEL_1543919f31dd46c6a967a412af4d21bd"
     }
    },
    "322cc0d27aea4458a9efe1c099943482": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0d781c1aae342ae949264fb561b7f5f",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_677cf9fc8f3d47c68e73bd4a3622132b",
      "value": 466062
     }
    },
    "373af73466be45f1bb216e6abbdb15be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3af999385a094ccc85eadf702ab69723": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e50074e3a054225842a6fe8ef936183",
      "placeholder": "​",
      "style": "IPY_MODEL_c2496080cc384158856247dd5581e751",
      "value": " 570/570 [00:00&lt;00:00, 13.4kB/s]"
     }
    },
    "3e50074e3a054225842a6fe8ef936183": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "405ea21b2692487298e89e719c6573b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd90fde29e26447c9eaeef9aaf15d01f",
      "placeholder": "​",
      "style": "IPY_MODEL_16b6fa8b34e24eedb7c6c80e9e57b5d2",
      "value": "Downloading: 100%"
     }
    },
    "41fe1073e1ea44669cdbef98949f9528": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43f942f812884ace90aa9b1dd41a3d9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45dcdc23c9b4481a9e40878e4dbfd585": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2a5689111c3420c9ed4114bb55cf0fc",
      "placeholder": "​",
      "style": "IPY_MODEL_d09c0639c5ef45a7823074eb648de6d7",
      "value": "Downloading: 100%"
     }
    },
    "4720c540eecc4dbe8fee8c9596f56933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "546361d1061243f5b7fc5692b1c9d4c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d80c6db4dfc4bf0aaea82350dffdc21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "602d9b5979084873996eab9784b98edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6497737147ce458b881bbb1b17492364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "677cf9fc8f3d47c68e73bd4a3622132b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b127ca5fb5a4d23b6c3ffe654cdfcaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c0f5454779b4baeafa828e961459116": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a39de3d6df814e98be1a65e049bff39e",
       "IPY_MODEL_ef706620a1e34c3a84de18a03a85f8cd",
       "IPY_MODEL_bfee36d5b72d469cb308f4007e1474a0"
      ],
      "layout": "IPY_MODEL_f17b3761a665477e8fdecfd23d267f87"
     }
    },
    "6f61782328964e4dbdea05bee3797ee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb792d8fbadc4bb6977a1986a9783e15",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b127ca5fb5a4d23b6c3ffe654cdfcaf",
      "value": 440473133
     }
    },
    "7e7c6e812fdc4b36a25096e9f176f75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e8aaf8e2a644b8384d07e86578e7624",
      "placeholder": "​",
      "style": "IPY_MODEL_d84069b7b1384beaa5100505f9233b2b",
      "value": "Downloading: 100%"
     }
    },
    "814534237a9144deb7d52e4ceffabc43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6497737147ce458b881bbb1b17492364",
      "placeholder": "​",
      "style": "IPY_MODEL_373af73466be45f1bb216e6abbdb15be",
      "value": " 420M/420M [00:10&lt;00:00, 36.1MB/s]"
     }
    },
    "8665ae01129141dfb7fdf5037a994c86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93f0f8c7257944e6ab97295f09821f1d",
      "placeholder": "​",
      "style": "IPY_MODEL_a61965e1ea50438c868778deb83db312",
      "value": " 28.0/28.0 [00:00&lt;00:00, 518B/s]"
     }
    },
    "89a7fc4f932041a7b76b2502096ede49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a4784a513b749d68959b4250a8a1593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93ded7574b56441eaa9b6aaaa30840f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93f0f8c7257944e6ab97295f09821f1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95d0e5c35a01408fa9bde956d0785d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_405ea21b2692487298e89e719c6573b0",
       "IPY_MODEL_0183e10141f049a1820827c9dfbd6c5a",
       "IPY_MODEL_8665ae01129141dfb7fdf5037a994c86"
      ],
      "layout": "IPY_MODEL_93ded7574b56441eaa9b6aaaa30840f3"
     }
    },
    "9d383d03f9304bce914326ea0e5d472b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a39de3d6df814e98be1a65e049bff39e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24f1399337dc4cf998e189d697b8e282",
      "placeholder": "​",
      "style": "IPY_MODEL_f3dea62699df437c945160dfc42afbd5",
      "value": "Downloading: 100%"
     }
    },
    "a61965e1ea50438c868778deb83db312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf065a9136dc4262a74c04f746b7c143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d383d03f9304bce914326ea0e5d472b",
      "placeholder": "​",
      "style": "IPY_MODEL_43f942f812884ace90aa9b1dd41a3d9a",
      "value": "Downloading: 100%"
     }
    },
    "bf7f2daca2264bd289670898d7ab10e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d80c6db4dfc4bf0aaea82350dffdc21",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89a7fc4f932041a7b76b2502096ede49",
      "value": 570
     }
    },
    "bfee36d5b72d469cb308f4007e1474a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcde074381814577a68122dd6e5adf30",
      "placeholder": "​",
      "style": "IPY_MODEL_4720c540eecc4dbe8fee8c9596f56933",
      "value": " 226k/226k [00:00&lt;00:00, 215kB/s]"
     }
    },
    "c2496080cc384158856247dd5581e751": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c38a9f17f85544b68ffdc8e501a95f20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45dcdc23c9b4481a9e40878e4dbfd585",
       "IPY_MODEL_322cc0d27aea4458a9efe1c099943482",
       "IPY_MODEL_e4669478d1474c4485c45907447d75a5"
      ],
      "layout": "IPY_MODEL_41fe1073e1ea44669cdbef98949f9528"
     }
    },
    "d01f56954ef74452b649def7b9adc937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d09c0639c5ef45a7823074eb648de6d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d84069b7b1384beaa5100505f9233b2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcde074381814577a68122dd6e5adf30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd90fde29e26447c9eaeef9aaf15d01f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0c07e3c6bd042c2a3dd2aed325f6759": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0d781c1aae342ae949264fb561b7f5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4669478d1474c4485c45907447d75a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_546361d1061243f5b7fc5692b1c9d4c5",
      "placeholder": "​",
      "style": "IPY_MODEL_602d9b5979084873996eab9784b98edc",
      "value": " 455k/455k [00:00&lt;00:00, 691kB/s]"
     }
    },
    "e8a66bb5d7614602ac93db9cac811135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf065a9136dc4262a74c04f746b7c143",
       "IPY_MODEL_6f61782328964e4dbdea05bee3797ee9",
       "IPY_MODEL_814534237a9144deb7d52e4ceffabc43"
      ],
      "layout": "IPY_MODEL_0504a8ce7be543a49611a424fcfc9ca7"
     }
    },
    "ef706620a1e34c3a84de18a03a85f8cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0c07e3c6bd042c2a3dd2aed325f6759",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff8facd357854c18976dea8f1621cae6",
      "value": 231508
     }
    },
    "f17b3761a665477e8fdecfd23d267f87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a5689111c3420c9ed4114bb55cf0fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3dea62699df437c945160dfc42afbd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb792d8fbadc4bb6977a1986a9783e15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff8facd357854c18976dea8f1621cae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
